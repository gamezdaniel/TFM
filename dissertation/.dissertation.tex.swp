% LaTeX template for the final master thesis

\documentclass[a4paper, 12pt]{book}
\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}
\usepackage{array}
\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{float}  									 % H for Figures positioning
\usepackage{graphicx}
\usepackage[latin1]{inputenc}
\usepackage{latexsym}  									 % LaTeX Logo
\usepackage{placeins}
\usepackage{textcomp}
\usepackage{times}
\usepackage{titlesec}
\usepackage[nottoc, notlot, notlof, notindex]{tocbibind} % Index options
\usepackage{url}

\pagestyle{plain}
\graphicspath{{./img/}}

\title{Implementation of a high availability solution based on Free Libre Open Source Software tools for Netnovation's Email and Collaboration System}
\author{Daniel H. G\'{a}mez V.}

\renewcommand{\baselinestretch}{1}  					% Interlining

\begin{document}

%=====================================
% COVER
%
\begin{titlepage}
  \begin{center}
  \begin{tabular}[c]{c c}
    \includegraphics[scale=0.25]{logo_vect.eps}
    \begin{tabular}[b]{l}
      \Huge
      \textsf{UNIVERSIDAD} \\
      \Huge
      \textsf{REY JUAN CARLOS} \\
    \end{tabular}
  \end{tabular}
  \vspace{3cm}
  
  \Large
  Master in Free Libre Open Source Software\\
  \vspace{0.2cm}
  \large
  Academic Course 2014/2015 \\
  \vspace{0.4cm}
  Master Thesis \\
  \vspace{1cm}
  \LARGE
  Implementation of a high availability solution based on Free Libre Open Source Software tools for Netnovation's Email and Collaboration System \\
  \vspace{2cm}
  \large
  Author: DANIEL H. G\'{A}MEZ V.\\
  Tutor: DR. GREGORIO ROBLES
  
  \end{center}
\end{titlepage}

%=====================================
% LICENSE
%
{\raggedleft
(c) 2015, Daniel H. G\'{a}mez V.\\
 daniel.gamez@gmail.com\\
 This work is licensed under a\\
 Creative Commons Attributions 3.0 License
  \begin{figure}[H]
    {\raggedleft
    \includegraphics[scale=0.80]{by-sa.png}\\
    \hfill http://creativecommons.org/licenses/by-sa/3.0/legalcode
    }
  \label{fig:logo}
  \end{figure}
}

%=====================================
% ABSTRACT
%
% \begin{abstract} This is the Abstract... \end{abstract}
% \chapter*{\centering Abstract}
\chapter*{Abstract}
\label{chap:abstract}
\addcontentsline{toc}{chapter}{Abstract}

This is the Abstract...

\noindent
Key words: Cluster, Corosync, CRM, DRBD, FOSS, F$\ell$OSS, High Availability, OCF, Pacemaker, Zimbra

%=====================================
% ACKNOWLEDGEMENTS
%
\chapter*{Acknowledgements}
\label{chap:acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}

These are the Acknowledgements..


%=====================================
% TERMINOLOGY
%
\chapter*{Terminology}
\label{chap:terminology}
\addcontentsline{toc}{chapter}{Terminology}

\begin{itemize}[label={}]
	\item API: Application Programming Interface
	\item CLI: Command Line Interface
	\item cLVM: clustered Logical Volume Manager
	\item CPU: Central Process Unit
	\item CRM: Cluster Resource Manager / Customer Relationship Management
	\item CTDB: Clustered Trivial Database
	\item DRBD: Distributed Replicated Block Device
	\item F$\ell$OSS: Free Libre Open Source Software
	\item GNU: GNU is Not Unix
	\item GPL:  General Public License
	\item GUI: Graphic User Interface
	\item HA: High Availability
	\item HAWK: HA Web Konsole
	\item HDD: Hard Disk Drive
	\item IPv4: Internet Protocol version 4
	\item ISO: International Organization for Standardization
	\item IT: Information Technologies
	\item KVM: Kernel-based Virtual Machine
	\item LGPL: Lesser General Public License
	\item LSB: Linux Standard Base
	\item OBS: openSUSE Build Service
	\item OCF: Open Cluster Framework
	\item OCFS: Oracle Cluster File System
	\item OpenBRR: Open Business Readiness Rating
	\item OS: Operating System
	\item PBX: Private Branch Exchange
	\item RHEL: Ret Hat Enterprise Linux
	\item SAN: Storage Area Network
	\item SCSI: Small Computer System Interface
	\item SLA: Service Level Agreement
	\item SME: Small and Medium Enterprises
	\item VPN: Virtual Private Network
	\item VPS: Virtual Private Server
	\item WAN: Wide Area Network
	\item YaST: Yet another Setup Tool
	\item ZCS: Zimbra Collaboration System
\end{itemize}


%=====================================
% CONTENTS
%
\tableofcontents  	% General Index
\listoffigures  	% Figures Index
\listoftables 		% Tables Index

%=====================================
% INTRODUCTION
%
\chapter{Introduction}
\label{chap:introduction}

\noindent With the growing use of cloud-oriented systems and the need for this information to be always available, online systems play an increasingly important role in our society nowadays. The technologies that support such schemes have evolved fleetingly and today there are numerous ways to get this kind of solutions, from proprietary software implementations, tools oriented to corporate environments that can be even based on F$\ell$OSS products and a variety of business models, to F$\ell$OSS standalone tools representing a robust solution to meet today's demands.\bigskip

\noindent Since 2004 Netnovation\texttrademark \ is a Venezuelan SME formed by a team of professionals in the areas of IT and telecommunications who adopted a business model based on consulting around F$\ell$OSS, providing system integration, timely development and Software as a Service (SaaS) cloud services.\bigskip

\noindent Due to a number of reasons that will be discussed throughout this dissertation, cloud services require a set of components to ensure the security, availability and reliability of data that is stored in data center facilities remotely accessible from internet. The study conducted here is focused specifically on the availability of the data that must be accessible to their applicants at all times.\bigskip

\noindent This study was undertaken in the one hand to test some aspects of business practices in the current technology market and the use of F$\ell$OSS as a key factor, and in the other one, to fulfill the demands of a business organization showing that business models around F$\ell$OSS are a fact.\bigskip

\noindent The way this dissertation is going to be organized is the following:\bigskip

\noindent Chapter 2 with the problem statement, setting why there is an issue in the current situation, what are the justification and motivation of this dissertation, also defining the objectives and the proper scope.\bigskip

\noindent Chapter 3 describing the most relevant related technologies around the possible solutions.\bigskip

\noindent Chapter 4 establishing the used methodology, supported by the OpenBRR and the Lazy User models.\bigskip

\noindent Chapter 5 showing the architecture of the company and its infrastructure, available hardware for operations, the network scheme, and an overview of the software supporting the current situation, all of this in order to understand how to adapt an actual solution.\bigskip

\noindent Chapter 6 describing the implemented technologies to achieve the solution for the problem stated.\bigskip

\noindent Chapter 7 will provide a detailed description of the actual implementation for the proposed solution.\bigskip

\noindent Chapter 8 with the results and discussion around the subject.\bigskip

\noindent Finally in Chapter 9 the conclusions and future work will be raised.\bigskip


%=====================================
% PROBLEM STATEMENT 
%
\chapter{Problem statement}
\label{chap:problem}

Business continuity in the field of information technology is supported in a large extent by the uninterrupted operation of the systems used in productivity tasks~\cite{ISO22399}. These systems must be fault tolerant, so that operations have the least possible impact in the event that an unexpected incident occurs.\bigskip

\noindent Nowadays there are increasingly more people and organizations using centralized remote systems that allow online access to resources and everyday services, this scheme is called cloud computing~\cite{MandG}. Through this type of services, end users whether individuals or corporations, are abstracted to support the infrastructure that this entails, giving responsibility to intermediary companies providing cloud services. So these intermediaries are the ones who must ensure the proper availability of the services, as well as factors such as communications security and redundancy of stored data, among many others.\bigskip

\noindent In particular Netnovation is a SME in the field of information technologies, which provides private cloud services from data storage to hosting virtual private servers (VPS), including e-mail and collaboration servers. The latter is precisely one of the mainstays for the operations of the company, which employs mainly F$\ell$OSS to its internal systems, specifically using the F$\ell$OSS e-mail and collaboration suite Zimbra\texttrademark\footnote{\ http://www.zimbra.com}. One of the main problems that Netnovation faces is to ensure the communication and workflow continuity that is carried through this collaboration tool, as well as meet the SLAs offered to its customers over this software.\bigskip

\noindent There are various software solutions offering high availability of services such as those provided by Zimbra, each one with its own legal implications, associated costs and implementation difficulty\footnote{\ Some of these solutions will be addressed in Chapter~\ref{chap:related}}. A valid alternative is the integration of multiple tools in the field of F$\ell$OSS providing a framework to ensure  continuity of systems operation or business continuity. By doing it this way it is possible to adapt the different requirements and use different technologies to provide the most consistent solution to what is desired.\bigskip

\noindent One key technology for this purpose is a cluster, in particular a high availability cluster, basically a a group of computers interconnected that work together trying to maintain a service up and running all the time.\bigskip

\noindent On previous occasions, Netnovation has managed to successfully consolidate most of its operations infrastructure adapting F$\ell$OSS, making it a wonderful idea to keep this scheme working. To achieve this, it is necessary to evaluate the state of the art in the field of systems that provide high availability, with the aim of offering an effective solution, all of this in accordance to the guidelines that have been proposed by the company.


\section{Justification / Motivation}
\label{sec:justification}

The factors that motivates this work are on one hand, give proper credit to business models based on F$\ell$OSS as those used by technology companies nowadays~\cite{Daffara2}, and on the other hand, show that private enterprise can be benefited by F$\ell$OSS, through a set of toolsets and mechanisms who allow obtaining robust solutions in accordance to technology needs.

\section{Objectives}
\label{sec:objectives}

The overall objectives are:

\begin{itemize}
	\item To frame the F$\ell$OSS business model used by Netnovation
	\item To show various current alternatives provided by F$\ell$OSS at the corporate level
	\item To adapt the proposed solution to the guidelines established by Netnovation
	\item To establish an initial point reference for implementing high availability private cloud services offered by Netnovation
\end{itemize}

\noindent The specific objectives are:

\begin{itemize}
	\item To implement a high availability solution based on F$\ell$OSS for the e-mail and collaboration system Zimbra used by Netnovation
	\item To describe the methodology used for the selection of the solution to be implemented
	\item To describe the process undertaken to implement the selected solution
	\item To perform tests in a controlled laboratory environment and validate the correct operation of the solution in order to promote it to a production environment
\end{itemize}


\section{Scope}
\label{sec:scope}

The solution to be implemented consists of F$\ell$OSS tools that allow its adaptation to the current infrastructure of Netnovation, they are not intended to replace the elements of the existing operations  platform.\bigskip

\noindent The methodology used for the selection of F$\ell$OSS tools that make the proposed solution is not intended to provide an exhaustive process that considers all possibilities in the area, but a flexible way that allows classify them qualitatively, justifying their choice through concrete metrics.\bigskip

\noindent Having successfully implemented a high availability solution on the e-mail and collaboration system used by Netnovation, this will serve as a reference for providing high availability to other enterprise systems, but these other configurations are not covered in this exercise.\bigskip


%=====================================
% PRECEDENTS / RELEATED TECHNOLOGIES / STATE OF THE ART
%
\chapter{Related technologies}
\label{chap:related}
A possible way to categorize high-availability technologies are Enterprise Solutions and F$\ell$OSS based tools, considering that usually the first ones have associated support plans over the whole provided solution, whereas the second ones provide support over its own tools, but not necessarily over the whole cluster implementation.

\section{Commercial Enterprise Cluster Software}
\label{sec:enterprise}

Some of these high-availability implementations have been proprietary since the beginning, either way at some point commercial companies realized that these technologies could be integrated into open systems thanks to licenses such as GNU/LGPL. Since then, as a business model strategy, companies such as Hewlett Packard, Red Hat Inc. and SUSE (a Novell company) provide support plans over open system platforms, considering their entire solution and charging for business licenses, generally on annual basis. In the following sections there is a description for each of them.

\subsection{HP Serviceguard}
\label{subsec:serviceguard}

Hewlett Packard (HP) claims the credit for the development of the first high availability solution for UNIX systems since 1990\footnote{\url{http://www.hpintelco.net/sglx/service.html}}. MC/ServiceGuard is a high-availability cluster software released for HP-UX and later for GNU/Linux systems. Since the first development of the software, HP has partnered with with companies such as Oracle or SAP to deploy high-availability in enterprise environments, as Figure~\ref{fig:enterprise} (took from \url{http://www.hpintelco.net}) shows.\bigskip

\begin{figure}
  \centering
  \includegraphics[scale=0.70]{commercial-ha.png}
  \caption[High-availability race in enterprise environments]{High-availability race in enterprise environments}
  \label{fig:enterprise}
\end{figure}

\noindent With the appearance of Linux in 1994 and its increasing popularity together with GNU, in 1999 HP released a Linux port called SG/LX, allowing high-availability features on it.\bigskip

\noindent Since 2001, companies Intel, Red Hat and HP joined efforts to produce the Red Hat Open Source Solutions Initiative\footnote{\url{http://www.hpintelco.net/hp-intel-redhat.htm}} (OSSI), with the aim to reduce partner's sale cycle by delivering enterprise reliable solutions to their customers. To some extent, realizing that their technologies could be strengthened by F$\ell$OSS.\bigskip

\noindent Some technical specifications of this product are the following:

\begin{itemize}
	\item Proprietary Licensing Model\footnote{http://h20564.www2.hp.com/hpsc/doc/public/display?docId=emr\_na-c02199685}
	\item Active/active, active/standby, and rotating standby cluster types
	\item Quorum Server support
	\item 32 Fibre Channel nodes, 2 single-path SCSI nodes, and 32 multipath SCSI nodes
	\item Operating Systems RHEL and SLES
	\item Hardware HP ProLiant ML, DL, and BL G7, Gen8 and Gen9 servers
	\item HP 3PAR, EVA, StoreSure, EMC VMAX, VNX storage
	\item ext3, ext4, NFS, XFS, VxFS, VxVM and btrfs filesystems
	\item Logical Volume Manager
\end{itemize}

\noindent The general overview of an HP Serviceguard architecture is shown in Figure~\ref{fig:serviceguard} (took from data sheet HP Serviceguard Solutions for Linux\footnote{http://h20195.www2.hp.com/v2/GetDocument.aspx?docname=4AA4-1792ENW}).

\begin{figure}
  \centering
  \includegraphics[scale=0.70]{hp-serviceguard.png}
  \caption[Basic HP Serviceguard for Linux cluster]{Basic HP Serviceguard for Linux cluster}
  \label{fig:serviceguard}
\end{figure}


\subsection{Red Hat Cluster Suite}
\label{subsec:rhcs}

At the end of the 90's Red Hat Inc. introduced its Enterprise Linux Advanced Server, designed specifically for use in enterprise environments to deliver superior application support, performance, availability and scalability. It included a high-availability clustering feature as part of the base product. Since then, the product has evolved to the current Red Hat Cluster Suite (RHCS), provided as a separately licensed product, also on top of Red Hat's Linux Enterprise Server.\bigskip

\noindent The RHCS has three major features, one of them is the Cluster Manager or \textbf{cman}, which adds functionality to, and is dependent upon other cluster stacks such as Corosync or OpenAIS. Is noteworthy that this is an adaptation of the Linux-HA project. Another key feature is the Resource Group Manager or \textbf{rgmanager}, a fully functional replacement for Pacemaker intended to work exclusively with RHCS. And also the IP Load Balancing (originally called Piranha), was originally developed by researchers at Oak Ridge National Laboratory, basically a text mining technology that Red Hat adapted in order to allow transparent load balancing and failover between servers\footnote{http://www.ornl.gov/connect-with-ornl/for-industry/partnerships/technology-licensing/licensing-opportunity-announcements/piranha}.\bigskip

\noindent The Cluster Manager, the Resource Group Manager and the IP Load Balancing are complementary high-availability technologies that can be used separately or in combination, depending on application requirements. Some of these technologies come from previous F$\ell$OSS projects and have been properly integrated into RHCS.\bigskip

\noindent Some technical details of this product are the following:

\begin{itemize}
	\item Support for up to 128 nodes (16 nodes on Red Hat Enterprise Linux 3, 4, 5, and 6)
	\item NFS, CIFS, GFS share and cluster filesystem managers
	\item File system and services failover support
	\item Fully shared storage subsystem
	\item Comprehensive data integrity
	\item SCSI and fiber channel support
	\item OCF and LSB resource agents
\end{itemize}

\noindent A general RHCS infrastructure is shown in Figure~\ref{fig:rhcs} (took from RHCS online documentation\footnote{\url{https://access.redhat.com/documentation/en-US/Red\_Hat\_Enterprise\_Linux/5/html/Cluster\_Suite\_Overview/images/9106.png}}).

\begin{figure}
  \centering
  \includegraphics[scale=0.70]{rhcs.png}
  \caption[RHCS Basic Infrastructure]{RHCS Basic Infrastructure}
  \label{fig:rhcs}
\end{figure}


\subsection{SUSE Linux Enterprise High Availability Extension}
\label{subsec:suseha}

This is an integrated suite of clustering technologies that enables the implementation of high availability over physical and virtual Linux clusters~\cite{TandS}. It allows monitoring, messaging, and cluster resource management, handling failover and load balancing of resources.\bigskip

\noindent As Red Hat did, other companies such as Novell took advantage of the high-availability demand over the enterprise sector and also adapted their own HA solution. This product is available as a paid add-on to SUSE Linux Enterprise Server GNU/Linux distribution, although in OpenSUSE many of these tools are included into the base system for free (without charge), with available repositories on OBS to provide newer versions of the packages for various GNU/Linux distributions\footnote{\url{https://en.opensuse.org/openSUSE:High_Availability}}.\bigskip

\noindent Among the main product features of this product are the following:

\begin{itemize}
	\item Multiple clustering scenarios, as active/active and active/passive configurations, as well as hybrid physical and virtual clusters
	\item Supports mixed clustering, physical and virtual Linux servers, based on Xen and KVM hypervisors
	\item Corosync messaging and membership layer, also Pacemaker cluster resource manager
	\item Storage and data replication supporting Fibre Channel or iSCSI SAN
	\item Cluster-aware file systems with GFS and OCFS, and cLVM as volume manager
	\item Supports replication through DRBD
	\item Samba clustering with CTDB
	\item Provides resource agent manager OCF
	\item GUI and CLI administration tools, such as YaST, HAWK and CRM
\end{itemize}

\noindent A SUSE HA cluster architecture is depicted in Figure~\ref{fig:suseha} (took from SUSE Linux High Availability Extension online documentation\footnote{https://www.suse.com/documentation/sle\_ha/book\_sleha/graphics/ha\_cluster\_components\_arch.png}).

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.50]{suse-ha.png}
  \caption[SUSE HA Cluster Components]{SUSE HA Cluster Components}
  \label{fig:suseha}
\end{figure}


\section{High availability F$\ell$OSS based tools}
\label{sec:hafloss}

% *** Include professional support offered by these software vendors ***

Some of these tools are possibly part of the enterprise products mentioned before, but they do not necessarily have an infrastructure providing corporate-oriented services. Either way, some of these F$\ell$OSS technologies are backed by professional support over their standalone software, giving rise to business models such as product specialists~\cite{Daffara1}.\bigskip

\noindent To understand the comprehensive solution that these tools are capable to provide, it is appropriate to understand briefly how an HA cluster works and how its components have evolved since the appearance of this concept.\bigskip

\noindent An HA cluster could be defined as a group of computers supporting server applications, ensuring its accessibility with a minimum of down-time. They operate by using specialized software that leverage the redundancy of computers and avoid single points of failure by implementing a cluster architecture~\cite{Weygant}.\bigskip

\noindent Then, as long as there is a service that must be maintained up and running and accessible to the users, computers require a way to communicate to one another and coordinate between each other to provide this service. Here computers are running GNU/Linux operating system and rely on TCP/IP network protocol, as well as network interconnected hardware to achieve the communication between nodes in the cluster.\bigskip

\noindent One of the earliest tools who managed this task was Heartbeat, as a daemon installed on each node, able to \textit{talk} to the other ones and share cluster related information. This task was referred to as the Cluster Messaging Layer or Group Communication System.\bigskip

\noindent Another important branch evolved, focused on the service or group of resources that the cluster is supposed to provide, for which Heartbeat incorporated a Cluster Resource Manager (CRM). A process to manage software resources, making use of scripts known as \textit{Resource Agents} responsible to perform actions depending on the status of each node.\bigskip

\noindent With these two distinct groups, one mostly concerned with the cluster messaging, and the other concerned with cluster resources, begins an important race of technologies seeking specialization on each area.\bigskip

\noindent Heartbeat, part of the Linux-HA project, was first released in 1999 under GNU GPL and GNU LGPL license, maintaining a set of building blocks for high availability cluster systems, including a cluster messaging layer, a bunch of resource agents for a variety of applications, a plumbing library and an error reporting toolkit. Around 2007 this project evolved to Pacemaker, integrating or allowing interaction with multiple F$\ell$OSS cluster stacks such as Corosync and OCF.\bigskip

\noindent Nowadays Pacemaker is a resource manager responsible for starting and stopping cluster services in a proper way. Combined with other tools is able to detect service-level failures and move resources between cluster nodes as needed, to ensure the smooth operation of the services.\bigskip

\noindent On the other hand Corosync is responsible for cluster membership, message passing and quorum, using the totem protocol 
for heartbeat, monitoring other node's health.\bigskip

\noindent Technologies such as OpenAIS (a software API) also compete into the HA race. It is a F$\ell$OSS implementation of the Application Interface Specification released under the terms of the Artistic License\footnote{ http://opensource.org/licenses/Artistic-2.0}, used to define how HA applications work together, trying to mask hardware, operating system, middleware and application-level failures.\bigskip

\noindent As in enterprise HA solutions, Cluster Managers are an important element within HA implementations, providing backend GUI or CLI software that runs on one or all cluster nodes, responsible for managing and controlling clustered services. Currently there are several F$\ell$OSS alternatives, including the ones listed in Table~\ref{table:cluster_managers}.

\begin{table}
  \centering
  \begin{tabular}{ | r m{10cm} | }

    \hline    
    \multicolumn{2}{|c|}{\textbf{Apache Mesos}}\\
    \hline
    Company: &  \\
    License: & Apache License Version 2.0 \\
    Website: & http://mesos.apache.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{Keepalived}}\\
    \hline
    Company: &  \\
    License: & GNU GPL v2+ \\
    Website: & http://keepalived.sourceforge.net/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
    \multicolumn{2}{|c|}{\textbf{Linux Cluster Manager}}\\
    \hline
    Company: &  \\
    License: & GNU GPL \\
    Website: & http://linuxcm.sourceforge.net/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{oneSIS}}\\
    \hline
    Company: &  \\
    License: & GNU GPL v2 \\
    Website: & http://onesis.org/ \\
    Description: &  \\
    Commercial support: &  \\

    \hline    
    \multicolumn{2}{|c|}{\textbf{Rocks Cluster Distribution}}\\
    \hline
    Company: &  \\
    License: & BSD 3-Clause like \\
    Website: & http://www.rocksclusters.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{SCMS.pro}}\\
    \hline
    Company: &  \\
    License: & Apache 2 License \\
    Website: & http://www.scms.pro/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
    \multicolumn{2}{|c|}{\textbf{Ultra Monkey}}\\
    \hline
    Company: &  \\
    License: & LGPL v2 \\
    Website: & http://www.ultramonkey.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{xCAT}}\\
    \hline
    Company: &  \\
    License: & Eclipse Public License \\
    Website: & http://xcat.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
  \end{tabular}
\caption{F$\ell$OSS Cluster Managers}
\label{table:cluster_managers}
\end{table}


% *** STORAGE ***
\noindent The cluster oriented filesystems are another key element regarding HA implementations. They provide data replication and fault tolerance, allowing operations continuity against incidents.

\noindent There are also several F$\ell$OSS alternatives to provide clustered storage, including the tools listed in Table~\ref{table:cluster_filesystems}. One of the most consolidated clustered storage technologies is DRBD, first released in 1999, it will be aborded in Chapter~\ref{chap:implemented}.

\begin{table}
  \centering
  \begin{tabular}{ | r m{10cm} | }

    \hline
    \multicolumn{2}{|c|}{\textbf{Apache Hadoop}}\\
    \hline
    Company: &  \\
    License: & Apache License 2 \\
    Website: & http://hadoop.apache.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{GFS}}\\
    \hline
    Company: &  \\
    License: & GPL \\
    Website: & http://sourceware.org/cluster/gfs/ \\
    Description: &  \\
    Commercial support: &  \\

    \hline
    \multicolumn{2}{|c|}{\textbf{GlusterFS}}\\
    \hline
    Company: &  \\
    License: & GPL v3 \\
    Website: & http://www.gluster.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
    \multicolumn{2}{|c|}{\textbf{Lustre FS}}\\
    \hline
    Company: &  \\
    License: & GPL \\
    Website: & http://lustre.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{MooseFS}}\\
    \hline
    Company: &  \\
    License: & GPL v2 \\
    Website: & https://moosefs.com/ \\
    Description: &  \\
    Commercial support: &  \\

    \hline    
    \multicolumn{2}{|c|}{\textbf{OCFS2}}\\
    \hline
    Company: &  \\
    License: & GPL \\
    Website: & https://oss.oracle.com/projects/ocfs2/ \\
    Description: & Oracle Cluster File System \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{XtreemFS}}\\
    \hline
    Company: &  \\
    License: & BSD License \\
    Website: & http://www.xtreemfs.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
  \end{tabular}
\caption{F$\ell$OSS Clustered Storage Technologies}
\label{table:cluster_filesystems}
\end{table}


%=====================================
% METHODOLOGY
%
\chapter{Methodology}
\label{chap:methodology}

The followed roadmap to achieve the objectives outlined is a set of guidelines and suggestions for the adoption of F$\ell$OSS within SMEs~\cite{Daffara1}, in the sense that a methodology is not an exact formula but a set of practices. At using this model, companies find a supporting guide from the initial selection and adoption of F$\ell$OSS within the IT infrastructure and even to the consolidation of business models around open source.\bigskip

\noindent On the one hand, the guidelines proposed by Daffara suggest a research method by collecting and read as much information related to the project is available and select the appropriate solution from a matching set that fulfill the requirements. On the other hand this dissertation also contemplates an open and standard framework to select the technologies to be used, this is the light weight model OpenBRR~\cite{Wasserman}, which performs a quick and flexible assessment over software tools considered.\bigskip

\noindent Additionally, and following a practical formal approach, applying a model of technology acceptance such as the Lazy User Model (LUM)~\cite{CandT1} is possible to frame the process by which are chosen the technological tools that will make up the solution that meets user requirements, which in this case is being represented by the company Netnovation. This model focuses on user needs and the demanded effort when selecting a solution to a problem from a set of possible solutions, ``According to the lazy user model, a user is likely to choose the solution that requires the least effort. The user examines this cost in terms of time, energy and money when considering how to use a new solution."~\cite{CandT2} The LUM proposes that technology acceptance is impacted by this principle.\bigskip



%=====================================
% ARCHITECHTURE
%
\chapter{Architecture}
\label{chap:architecture}

In order to provide IT services to customers, considering software services hosted in an on-line remote location, Netnovation requires a proper software and hardware infrastructure to operate. Some of these services are from VPS, Data Storage Systems, Customer Relationship Management Systems (CRM), Email and Collaboration Systems, to Voice over IP PBX systems. Both parties, software infrastructure and product services offered are based on F$\ell$OSS.\bigskip

\noindent In particular the Zimbra server to which a high availability schema is been configured, resides into this architecture, and it is consistent with the company's principles and business model, which is why it is useful to understand the environment to which it belongs.


\section{Company Infrastructure}
\label{sec:infrastructure}

Currently the services are offered from two DataCenters (DC1 and DC2) geographically distributed with the aim of guarantee data redundancy. Assuming that communication with the main DC is lost, it has been defined a procedure that allows the restoration of services in the other DC, with the disadvantage that it is a manual procedure that requires administrators intervention.

\section{Existent Hardware}
\label{sec:hardware}

Each DC has an average of seven Dell PowerEdge\texttrademark \ Racked Servers with different capacities, interconnected via communication devices that provide various services analogously.\bigskip

\noindent There is a Dell PowerEdge 2850/2950 server serving as firewall and main router on each DC. On the one hand it has a WAN 1000Mbps interface, which is connected through UTP Cat-6 wired to 24 PoE ports Switches Netgear FS728TPv1 Gigabit. Physical servers are installed in 20U rack cabinets. These servers range from models Dell PowerEdge 1950, R510 to R710, have Intel Xeon CPUs within 24 and 64 cores, count with 8 to 64GB of RAM, and also have SCSI HDD with capacities between 100GB and 2,5TB.


\section{Company Network Scheme}
\label{sec:networkscheme}

The housing services leased by the providers offer a pool of public IPv4 addresses that are handled by the main router on each DC facility. DC1 and DC2 are interconnected by a VPN through WAN, each of them associated to a different private Class B network internally. To the LAN Ethernet ports of the switches are connected the physical servers of the private network with transfer speed rates of 100/1000Mbps. The overall interconnection scheme can be appreciated in Figure~\ref{fig:network}.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.50]{network_scheme.png}
  \caption[Network interconnection scheme]{Network interconnection scheme}
  \label{fig:network}
\end{figure}


\section{Software Supporting the Infrastructure}
\label{sec:software}

In Table~\ref{table:technologies} there are some software solutions currently used by Netnovation that are related with the required architecture to provide IT cloud-oriented services, with a brief description and legal licensing information for each one of them.

\begin{table}
  \centering
  \begin{tabular}{ | r m{10cm} | }
  
    \hline    
    \multicolumn{2}{|c|}{\textbf{UTM Endian Firewall}}\\
    \hline
    Company: & Endian S.r.l. \\
    Industry: & Unified Threat Management \\
    License: & GNU GPL \\
    Website: & endian.com \\
    Description: & A linux security distribution with full featured Unified Threat Management functionality. Include a stateful packet inspection firewall, application-level proxies for various protocols, antivirus support, virus and spam-filtering for email traffic, content filtering of Web traffic, also an OpenVPN solution. Distribution based on Red Hat. \\
    Supported Platforms: & GNU/Linux \\
    Commercial support: & annual subscription \\
    
    \hline
    \multicolumn{2}{|c|}{\textbf{Proxmox VE}}\\
    \hline
    Company: & Proxmox Server Solutions GmbH \\
    Industry: & Server Virtualization \\
    License: & GNU Affero and GPLv3 \\
    Website: & pve.proxmox.com \\
    Description: & Virtualization management solution for servers, based on KVM and containers Server Virtualization Platform, provides KVM and OpenVZ hypervisors. Distribution based on Debian. \\
    Supported Platforms: & GNU/Linux \\
    Commercial support: & annual subscription \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{FreeNAS}}\\
    \hline
    Company: & iXsystems, Inc. \\
    Industry: & Computer Storage \\
    License: & BSD 2-Clause \\
    Website: & freenas.org \\
    Description: & Network-attached storage server, supporting many network and storage protocols such as Samba and NFS. Also supports ZFS. Distribution based on FreeBSD. \\
    Supported Platforms: & BSD Unix \\
    Commercial support: & custom quotes and support tickets \\
    \hline
    \multicolumn{2}{|c|}{\textbf{Zabbix}}\\
    \hline
    Company: & Zabbix SIA \\
    Industry: & IT Monitoring \\
    License: & GNU GPLv2 \\
    Website: & zabbix.com \\
    Description: & Solution for monitoring of networks, applications and databases. \\
    Supported Platforms: & GNU/Linux \\
    Commercial support: & custom quotes and support tickets \\
    \hline
  \end{tabular}
\caption{Software Supporting the Infrastructure}
\label{table:technologies}
\end{table}


%=====================================
% IMPLEMENTED TECHNOLOGIES
%
\chapter{Technological background}
\label{chap:implemented}

The following software tools represent the key elements on which it has been possible to implement a comprehensive high availability solution, some of them mentioned in Section~\ref{sec:hafloss}.

\begin{itemize}
	\item Red Hat Enterprise Linux Server
\end{itemize}

\noindent GNU/Linux enterprise-oriented distribution providing a very stable base system, vast documentation and proper support from manufacturer, released as F$\ell$OSS mainly under the terms of the GNU Lesser General Public License 2.1, except for some optional components. In order to be specific in this exercise, the Linux kernel 2.6.32-431.el6.x86\_64 that is included by the RHEL version 6.5 was used.

\begin{itemize}
	\item Zimbra Collaboration System (ZCS)
\end{itemize}

\noindent Server and client collaboration software, supporting e-mail, contacts, calendar, documents, push synchronization, and many other enterprise features related to groupware. The software is F$\ell$OSS released under the terms of the Common Public Attribution License version 1 and the GNU General Public License version 2 (GPLv2). The exact version implemented was ZCS FOSS  8.0.7\_GA\_6021.RHEL6\_64.

\begin{itemize}
	\item Distributed Replicated Block Device (DRBD)
\end{itemize}

\noindent A distributed replicated storage system for Linux, implemented as several userspace management applications and shell scripts, used to provide data redundancy. Works on top of block devices, such as hard disk partitions or LVM logical volumes, mirroring each data block that it is written to disk to the peer node.

\begin{itemize}
	\item Corosync
\end{itemize}

\noindent It is released as F$\ell$OSS under the 3-clause BSD License. This software provides features based on C programming language implementing high availability within applications, through virtual synchrony for replicated state machines, simple availability handling responsible for applications restart when fail, it keeps configuration and statistics in a memory database providing the ability to set, retrieve, and receive change notifications of information, and a quorum system that notifies applications when it is achieved or lost.

\begin{itemize}
	\item Pacemaker
\end{itemize}

\noindent A F$\ell$OSS high availability resource manager software released under GNU GPLv2. This software was part of the Linux-HA project until 2007, then was split out to be its own project. It implements APIs for resources control, including the Open Cluster Framework (OCF). It is used on computer clusters since 2004.

\begin{itemize}
	\item Cluster Resource Manager Shell (CRMsh\footnote{http://crmsh.github.io}) and Pacemaker Configuration System (PCS\footnote{https://github.com/feist/pcs})
\end{itemize}

\noindent Initially, the CRMsh was distributed as part of the Pacemaker project, but it was split into its own separate project in 2011. Also as CRMsh, PCS is a command-line interface to the Pacemaker cluster resource management stack.

\begin{itemize}
	\item Cluster Configuration System (CCS)
\end{itemize}

\noindent Manages the cluster configuration and provides information to other cluster components. Runs in each cluster node and makes sure that the cluster configuration file in each cluster node is up to date. In Figure~\ref{fig:ccs} (\small {took from \url{https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/4/html/Cluster_Suite_Overview/images/}}) is represented a CCS overview.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.50]{ccs-overview.png}
  \caption[CCS overview]{CCS overview}
  \label{fig:ccs}
\end{figure}

\begin{itemize}
	\item Cluster Manager (CMAN\footnote{https://www.sourceware.org/cluster/cman/})
\end{itemize}

\noindent A set of kernel patches and a userspace program, formed by a Connection Manager (cnxman) and a Service Manager (sm). The first one handles membership, messaging, quorum, event notification and transitions, and the second one is responsible for instances of external systems. It combines some functionalities provided by CRMsh, PCS and CCS.


%=====================================
% IMPLEMENTATION
%
\chapter{Implementation}
\label{chap:implementation}

This section is intended to provide technical documentation in the process of implementing high availability in a F$\ell$OSS Zimbra Collaboration System (ZCS). The scope of this implementation is framed by the following software components and versions:

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\item Red Hat Enterprise Linux Server release 6.5 (Santiago)
	\item GNU/Linux 2.6.32-431.el6.x86\_64
	\item zcs 8.0.7\_GA\_6021.RHEL6\_64 FOSS edition
	\item drbd 8.4.3-33
	\item corosync 1.4.5-2.2
	\item pacemaker 1.1.10-14
	\item pcs 0.9.90-2
	\item crmsh 1.2.5-0
	\item ccs 0.16.2-69
	\item cman 3.0.12.1-59
\end{itemize}

\noindent The defined cluster consists of two nodes which will be referenced as Astapor and Braavos in the domain got.com (as in the novel Game of Thrones). These nodes are virtual machines hosted on two Proxmox Virtual Environment servers based on KVM virtualization, which are installed on separate physical machines in the same LAN to avoid single point of failure. The proposed scheme is conceptually similar to the observed in Figure~\ref{fig:ha-cluster}.

\FloatBarrier
\begin{figure}
  \centering
  \includegraphics[scale=0.50]{two_nodes_ha_cluster.png}
  \caption[Two nodes HA cluster]{Two nodes HA cluster}
  \label{fig:ha-cluster}
\end{figure}


\section{Operating system considerations}
\label{sec:considerations}

The configuration must be similar in both nodes. In Table~\ref{table:os} is shown the configuration selected for the current implementation. It should be take into consideration that it is not necessary to format partitions for devices vdb1 or vdc1 during OS install.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r l l l | }
  
    \hline    
    \multicolumn{4}{|c|}{\textbf{RHEL 6.5 x86\_64}}\\
    \hline
    Disk Partitions: & /              & 10 Gb  & \\
                     & /boot          & 100 Mb & \\
                     & /opt/zimbra    & 8 Gb   & (/dev/vdb1)\\
                     & drbd meta-data & 150 Mb & (/dev/vdc1)\\
    CPU:             & 1              &        & \\
    RAM:			 & 2 Gb 		  &        & \\
    \hline
  \end{tabular}
\caption{Operating system configuration}
\label{table:os}
\end{table}


\subsection{FQDN hostnames and IP addresses}
\label{sec:fqdn}

Table~\ref{table:fqdn} shows the current configuration for the virtual IP address shared by the two nodes, and for the primary IP address on each node.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r c l | }
    \hline    
    Split DNS IP: & 172.17.18.190 & zcs-ha.got.com \\
	Astapor:      & 172.17.18.191 & astapor.got.com \\
	Braavos:      & 172.17.18.192 & braavos.got.com \\
    \hline
  \end{tabular}
\caption{FQDN hostnames and IP addresses}
\label{table:fqdn}
\end{table}

\noindent On both nodes, /etc/hosts file should contain at least the entries described in Table~\ref{table:hosts} :

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l l l | }
    \hline    
    127.0.0.1     & localhost.localdomain & localhost \\
    127.0.0.1     & zcs-ha.got.com        & zcs-ha\\
	172.17.18.190 & astapor.got.com       & astapor\\
	172.17.18.191 & braavos.got.com       & braavos\\
    \hline
  \end{tabular}
\caption{/etc/hosts file}
\label{table:hosts}
\end{table}

\noindent A useful command to handle hostname changes in RHEL:\\
\indent service hostname restart\bigskip


\subsection{Network}
\label{sec:network}

\begin{itemize}
	\item Internet Protocol version 4 (IPv4)
\end{itemize}

\noindent Set the proper network parameters in /etc/sysconfig/network-scripts/ifcfg-eth0 file on each server, as described in Table~\ref{table:ifcfg}.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l | l | }
    \hline
    \multicolumn{1}{|c|}{Astapor} & Braavos \\
    \hline
    DEVICE=eth0 & DEVICE=eth0 \\
    HWADDR=26:34:99:65:d7:77 & HWADDR=26:34:99:65:d7:78\\
    TYPE=Ethernet & TYPE=Ethernet\\
    ONBOOT=yes & ONBOOT=yes\\
    NM\_CONTROLLED=no & NM\_CONTROLLED=no\\
    BOOTPROTO=none & BOOTPROTO=none\\
    IPADDR=172.17.18.191 & IPADDR=172.17.18.192\\
    NETMASK=255.255.255.0 & NETMASK=255.255.255.0\\
    GATEWAY=172.17.18.1	& GATEWAY=172.17.18.1\\
	DNS1=127.0.0.1 & DNS1=127.0.0.1\\
	IPV6INIT=no & IPV6INIT=no\\
	USERCTL=no & USERCTL=no\\
    \hline
  \end{tabular}
\caption{/etc/sysconfig/network-scripts/ifcfg-eth0 file}
\label{table:ifcfg}
\end{table}


\noindent Set the correct Netmask and Gateway, so servers are able to reach internet addresses, also disable the firewall or allow the http and ftp outgoing rules on it. The primary DNS server will be configured later to be the localhost, with forwarding to external DNS servers.\bigskip

\noindent Some useful commands to manipulate and consult the network service on RHEL:\\
\indent service network restart\\
\indent /etc/init.d/network restart\\
\indent ifconfig eth0 down; ifconfig eth0 up\\
\indent ifdown eth0; ifup eth0\\
\indent ifconfig\\
\indent ip addr show

\begin{itemize}
	\item NTP
\end{itemize}

\noindent Required RPM packages to sincronize cluster nodes through network time protocol: ntp, ntpdate.\bigskip

\noindent Set the proper NTP parameters in /etc/ntp.conf file on each server, so both nodes share the same date and time, as shown in Table~\ref{table:ntp}.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l l | }
    \hline
    driftfile & DEVICE=eth0/var/lib/ntp/drift \\
	restrict & default kod nomodify notrap nopeer noquery \\
    restrict & 127.0.0.1 \\
    server & 172.17.18.1 \\
    includefile & /etc/ntp/crypto/pw \\
    keys & /etc/ntp/keys \\
    \hline
  \end{tabular}
\caption{/etc/ntp.conf file}
\label{table:ntp}
\end{table}


\noindent Some useful commands to manipulate and consult NTP service on RHEL are:\\
\indent service ntpd restart \\
\indent ntpstat \\
\indent ntpq -pn \\
\indent date

\begin{itemize}
	\item BIND
\end{itemize}

\noindent Required RPM packages for domain name resolution: bind, bind-utils.\bigskip

\noindent A primary DNS server configured on each server is crucial, or alternatively a remote centralized DNS server on the LAN with the whole configuration. Here is considered the first option. Table~\ref{table:named} shows the content of /etc/named.conf file.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l | }
    \hline
	  zone ``got.com." IN \{ \\
       type master; \\
       file ``got.com.db"; \\
       \}; \\
    \hline
  \end{tabular}
\caption{/etc/named.conf file}
\label{table:named}
\end{table}


\noindent Astapor node holds /var/named/got.com.db file, with the content described in Table~\ref{table:bindastapor}. Dot characters at the end of hostnames are not a typo, they should be included so that the configuration is correct, and must be absent in the case of IP addresses.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l l l l l | }
    \hline
	                  & IN & 1H & NS    & zcs-ha.got.com.\\
                      & IN & 1H & MX 5  & zcs-ha.got.com.\\
      zcs-ha          & IN & 1H & A     & 172.17.18.190\\
      astapor         & IN & 1H & A     & 172.17.18.191\\
      astapor.got.com & IN &    & CNAME & zcs-ha.got.com.\\
    \hline
  \end{tabular}
\caption{/etc/named.conf file}
\label{table:bindastapor}
\end{table}

\noindent A similar got.com.db file must be set on braavos node replacing the corresponding hostname and IP address. Leaving zcs-ha entries without changes in both nodes.\bigskip

\noindent Some useful commands to handle and request BIND service on RHEL are:\\
\indent named-checkconf -z \\
\indent service named restart \\
\indent service named status \\
\indent dig -t ANY got.com \\
\indent nslookup astapor.got.com

\subsection{ZCS dependencies}
\label{sec:zcsdeps}

As requirement for ZCS, the following RPM packages must be installed in the OS:

\begin{itemize}
	\item nc
	\item sudo
	\item libidn 
	\item gmp 
	\item libaio 
\end{itemize}

\noindent Some other suggested RPM packages are:

\begin{itemize}
	\item perl-5.10.1
	\item sysstat 
	\item sqlite 
\end{itemize}

\noindent The postfix daemon must be turned off and excluded from boot start-up:\\
\indent service postfix stop\\
\indent chkconfig postfix off


\section{DRBD}
\label{sec:drbd}

\noindent The Distributed Replicated Block Device (DRBD) provides a mirrored storage required for the HA environment.


\subsection{Initial configuration}
\label{sec:initialconf}

The following actions must be performed in parallel on both nodes, except in those cases where otherwise specified.

\begin{itemize}
	\item Ensure to adapt hostname to `astapor' on the primary node and `braavos' on the secondary node.
	
	\item Install RPM packages:\\
			drbd-kmdl-2.6.32-431.el6-8.4.3-33.el6.x86\_64\\
			drbd-8.4.3-33.el6.x86\_64
	
	\item Leave /etc/drbd.conf and /etc/drbd.d/global\_common.conf files by default.
	
	\item Add /etc/drbd.d/optzimbra.res file with the content described in Table~\ref{table:resource}:

%%%%%%%%%%%%%%%%%%%% MODIFY TABLE %%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l l l l l | }
    \hline
      zcs-ha          & IN & 1H & A     & 172.17.18.190\\
      astapor         & IN & 1H & A     & 172.17.18.191\\
    \hline
  \end{tabular}
\caption{/etc/drbd.d/optzimbra.res file}
\label{table:resource}
\end{table}
	
	\item Remove from /etc/fstab file any reference to /dev/vdb1 or /dev/vdc1 devices, as drbd is going to handle its mounting.
	
	\item Initialize data and metadata disks:\\
		\indent dd if=/dev/zero of=/dev/vdb1 bs=1K count=100\\
		\indent dd if=/dev/zero of=/dev/vdc1 bs=1K count=100
	
	\item Start DRBD module:\\
		modprobe drbd
	
	\item Create resource:\\
		drbdadm create-md optzimbra
	
	\item Execute first DRBD synchronisation on astapor:\\
		drbdadm up optzimbra\\
		drbdadm primary --force optzimbra\\
		drbdadm --discard-my-data connect optzimbra

	\item It is possible to check synchronisation status with:\\
		watch cat /proc/drbd

	\item Final output will show:\\
		ds:UpToDate/UpToDate

	\item Verify current roles:\\
		drbdadm role optzimbra\\
		It will show 'Primary/Secondary' on astapor \\
		and 'Secondary/Primary' on braavos node.

	\item Now make the filesystem on astapor:\\
		mkfs.ext4 /dev/drbd0

	\item Then demote node to secondary, by executing only on astapor:\\
		drbdadm secondary optzimbra

	\item Promote node to primary, by executing only on braavos:\\
		drbdadm primary optzimbra

	\item Make the filesystem on braavos:\\
		mkfs.ext4 /dev/drbd0

\end{itemize}

\noindent Now it is necessary to revert the roles back, making braavos the secondary node and astapor the primary one, with the procedure described in section~\ref{sec:initialconf}.


\subsection{DRBD Split Brain Recovery}
\label{sec:splitbrain}

\noindent Assuming that the primary node is still consistent, and the secondary node has an inconsistent state, it would be necessary to recover data loss. The following actions will allow to recover the data corrupted in secondary node.

\begin{itemize}
	\item In both nodes:\\
		drbdadm disconnect optzimbra
	
	\item In the secondary node:\\
		drbdadm secondary optzimbra\\
		drbdadm connect -\ -discard-my-data optzimbra
	
	\item In the primary node:\\
		drbdadm connect optzimbra
	
	\item Finally it is possible to check the sync status, running the command ``cat /proc/drbd", which is going to show a message similar to this:\\
		cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r- \ - \ - \ -

\end{itemize}

\section{ZCS}
\label{sec:zcs}

Here will be fully installed ZCS on astapor but just a dummy installation on braavos, since DRBD will replicate the data to the other node. Download and place ZCS installation file in astapor and braavos filesystems. It can be found at \url{http://www.zimbra.com/downloads/os-downloads.html}. In order to complete a full install on a single server, the following resource will be useful:\\
\url{http://files.zimbra.com/website/docs/8.5/Zimbra_OS_Quick_Start_8.5.0.pdf}


\subsection{ZCS full install on primary node}
\label{sec:fullinstall}

The following actions must be performed sequentially on \textit{\textbf{astapor}}.

\begin{itemize}
	\item Create directory for ZCS:\\
		mkdir   /opt/zimbra
	
	\item Mount DRBD device on ZCS mount point:\\
		mount   /dev/drbd0   /opt/zimbra
	
	\item Check mounted device:\\
		df\  \textbar \ grep zimbra\\
		mount\  \textbar \ grep zimbra
	
	\item Set manual virtual link configuration temporally:\\
		ifconfig eth0:1 inet 172.17.18.190 netmask 255.255.255.0
	
	\item Set split DNS hostname temporally:\\
		hostname zcs-ha.got.com\\
		It is also recommendable to change /etc/sysconfig/network file.
	
	\item Unpack ZCS installer and proceed with full installation:\\
		./install.sh
	
	\item Leave all packages to install by default, and follow the process.
	
	\item When prompted for domain name change, select ``Yes" and then provide: got.com
	
	\item On ``Main Menu" section, set admin user password by browsing through option 3 and then 4:\\
		``Password for admin@zcs-ha.got.com (min 6 characters)"
	
	\item Apply configuration and advance until ZCS setup process is completed:\\
		``Configuration complete - press return to exit"
	
	\item Check ZCS status:\\
		service zimbra status
	
	\item Stop ZCS:\\	
		service zimbra stop
	
	\item Umount DRBD device:\\
		umount   /opt/zimbra
	
	\item Set original DNS hostname:\\
		hostname astapor.got.com\\
		* Revert change in /etc/sysconfig/network file if needed.
	
	\item Delete temporal virtual link configuration:\\
		ifconfig eth0:1 down
	
	\item Demote astapor to secondary DRBD, and continue with section~\ref{sec:dummyinstall}:\\
		drbdadm secondary optzimbra
		
\end{itemize}


\subsection{ZCS dummy install on secondary node}
\label{sec:dummyinstall}

\noindent The following actions must be performed sequentially on braavos.

\begin{itemize}
	\item Promote braavos to primary DRBD:\\
		drbdadm primary optzimbra
		
	\item Create directory for ZCS:\\
		mkdir   /opt/zimbra
		
	\item Mount DRBD device on ZCS mount point:\\
		mount   /dev/drbd0   /opt/zimbra
	
	\item Check mounted device:\\
		df\  \textbar \ grep zimbra\\
		mount\  \textbar \ grep zimbra
		
	\item Unpack ZCS installer and proceed with a dummy installation:\\
		./install.sh -s
		
	\item Stop ZCS:\\
		service zimbra stop
	
	\item Umount DRBD device:\\
		umount   /opt/zimbra
		
	\item Demote braavos back to secondary DRBD:\\
		drbdadm secondary optzimbra
		
	\item Promote astapor back to primary DRBD, executing from astapor node:\\
		drbdadm primary optzimbra
		
\noindent At this point DRBD has to synchronize data from primary node, so check the status until it is done:\\
	watch cat /proc/drbd
	
\end{itemize}

\section{OCF}
\label{sec:ocf}

\noindent Open Cluster Framework, standard scripts to control services such as ZCS. Following actions must be performed in both nodes.

\begin{itemize}
	\item Add btactic zimbra script to /usr/lib/ocf/resource.d/btactic/zimbra. The source code of this script is included in the Appendix~\ref{app:appendix1}.\\

	\item Also create the following symbolic link:\\
		ln \ -s \ /usr/lib/ocf/resource.d/btactic/zimbra \ /usr/lib/ocf/resource.d/heartbeat/
\end{itemize}

\noindent In section~\ref{sec:pacemaker} this file will be referenced.

\section{Pacemaker}
\label{sec:pacemaker}

\noindent Resource manager, starts and stops services orderly.

\begin{itemize}
	\item Install the required RPM packages:\\
		pacemaker-cluster-libs-1.1.10-14.el6.x86\_64\\
		pacemaker-libs-1.1.10-14.el6.x86\_64\\
		pacemaker-cli-1.1.10-14.el6.x86\_64\\
		pacemaker-1.1.10-14.el6.x86\_64\\
		cman-3.0.12.1-59.el6.x86\_64\\
		crmsh-1.2.5-0.el6.x86\_64\\
		ccs-0.16.2-69.el6.x86\_64\\
		resource-agents-3.9.2-40.el6\_5.7.x86\_64
\end{itemize}

\noindent Usually it is difficult to obtain the required RPM's for RHEL, so an alternative is to add CentOS repository by editing /etc/yum.repo.d/centos.repo file with the content described in Table~\ref{table:centosrepo}.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r c l | }
    \hline
      \multicolumn{3}{|c|}{[centos-6-base]}\\
      \hline
      name &=& CentOS-\$releasever - Base\\
      mirrorlist &=& http://mirrorlist.centos.org/?release=6.5\&arch=x86\_64\&repo=os\\
      enabled &=& 0\\
      gpgcheck &=& 0\\
      baseurl &=& http://mirror.centos.org/centos/6.5/os/x86\_64/\\
    \hline
  \end{tabular}
\caption{/etc/yum.repo.d/centos.repo file}
\label{table:centosrepo}
\end{table}

\begin{itemize}
	\item Then update and install the packages:\\
		yum \ install \ \textminus \textminus enablerepo=centos-6-base \ pacemaker \ pcs.noarch \ cman \textbackslash \\
		ccs \ resource-agents \ crmsh
\end{itemize}


\noindent There are two ways to interact with Pacemaker configuration. The first one is using the crmsh interpreter, starting the crm shell with ``crm" command, and then providing configuration sentences. For instance:\bigskip

	[root@astapor \textasciitilde]\# crm\\
	\indent crm(live)\# help\\
	\indent crm(live)\# quit\\

\noindent Another way would be through \textit{\textbf{pcs}} and \textit{\textbf{ccs}} instructions directly from a linux tty in a bash session. Following is going to be used this way to configure the cluster, executing the commands only on the primary node.

\begin{itemize}
	\item Create the cluster:\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus createcluster \ zcsCluster
\end{itemize}

\begin{itemize}
	\item Add the nodes:\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addnode \ astapor.got.com\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addnode \ astapor.got.com
\end{itemize}

\begin{itemize}
	\item Set fencing to defer to Pacemaker:\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addfencedev \ pcmk \ agent=fence\_pcmk\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addmethod \ pcmk-redirect \ astapor.got.com\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addmethod \ pcmk-redirect \ braavos.got.com\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addfenceinst \ pcmk \ astapor.got.com \textbackslash \\ pcmk-redirect \ port=astapor.got.com\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addfenceinst \ pcmk \ braavos.got.com \textbackslash \\ pcmk-redirect \ port=braavos.got.com
\end{itemize}


\begin{itemize}
	\item Disable CMAN quorum:\\
		This will let the cluster function if only one node is up, and it is necessary to be performed in both nodes.\\
		echo \ ``CMAN\_QUORUM\_TIMEOUT=0" \ \textgreater \textgreater \ /etc/sysconfig/cman
\end{itemize}

\begin{itemize}
	\item Start Pacemaker Cluster:\\
		pcs \ cluster \ start \ \textminus \textminus all\\
		Also equivalent to execute on each node,\\
		``service pacemaker start" or ``pcs cluster start"
\end{itemize}

\begin{itemize}
	\item Copy cluster file to secondary node:\\
		scp \ -p \ /etc/cluster/cluster.conf \ braavos:/etc/cluster/
\end{itemize}

\begin{itemize}
	\item Check Pacemaker cluster status:\\
		pcs \ status\\
		crm\_mon \ -1
\end{itemize}


\begin{itemize}
	\item Show current cluster config:\\
		pcs \ config\\
		pcs \ property\\
		crm \ configure \ show
\end{itemize}


\begin{itemize}
	\item Check configuration validity:\\
		crm\_verify \ -L \ -V
\end{itemize}


\begin{itemize}
	\item Disable STONITH (a type of fencing):\\
		pcs \ property \ set \ stonith-enabled=false
\end{itemize}


\begin{itemize}
	\item Ignore Quorum Policy:\\
		pcs \ property \ set \ no-quorum-policy=ignore
\end{itemize}


\begin{itemize}
	\item Set reconnect attempt:\\
		pcs \ property \ set \ migration-threshold=1 \ \textminus \textminus force
\end{itemize}


\begin{itemize}
	\item Set stickiness:\\
		pcs \ property \ set \ resource-stickiness=100 \ \textminus \textminus force
\end{itemize}

\bigskip

\noindent Now, it is going to be used the crmsh interpreter, starting it with the following command:\\
	\indent crm \ configure

\bigskip
	
\begin{itemize}
	\item Add floating IP address resource (Virtual IP - VIP):\\
		pcs \ resource \ create \ VIP1 \ IPaddr2 \ ip=172.17.18.190 \ broadcast=172.17.18.255 \ \textbackslash \\
			nic=eth0 \ cidr\_netmask=24 \ iflabel=VIP1 \ op \ monitor \ interval=30s \ timeout=30s
\end{itemize}


\begin{itemize}
	\item Define DRBD cluster resource:\\
		configure \ primitive \ drbd \ ocf:linbit:drbd \ params \ \textbackslash \\
			drbd\_resource=optzimbra \ \textbackslash \\
			op \ monitor \ role=Master \ interval=60s \ \textbackslash \\
			op \ monitor \ role=Slave \ interval=50s \ \textbackslash \\
			op \ start \ role=Master \ interval=60s \ timeout=240s \ \textbackslash \\
			op \ start \ role=Slave \ interval=0s \ timeout=240s \ \textbackslash \\
			op \ stop \ role=Master \ interval=60s \ timeout=100s \ \textbackslash \\
			op \ stop \ role=Slave \ interval=0s \ timeout=100s
\end{itemize}

\begin{itemize}
	\item Define DRBD Zimbra data clone:\\
		configure \ ms \ drbd\_ms drbd \ \textbackslash \\
		meta \ master-max=1 \ master-node-max=1 \ \textbackslash \\
		clone-max=2 \ clone-node-max=1 \ notify=true
\end{itemize}

\begin{itemize}
	\item Define Zimbra service resource:\\
		configure \ primitive \ zcs\_service ocf:btactic:zimbra \ \textbackslash \\
		op \ monitor \ interval=2min \ timeout="40s" \ \textbackslash \\
		op \ start \ interval="0" \ timeout="360s" \ \textbackslash \\
		op \ stop \ interval="0" \ timeout="360s"
\end{itemize}

\begin{itemize}
	\item Define Zimbra cluster filesystem resource:\\
		configure \ primitive \ zcs\_fs \ ocf:heartbeat:Filesystem \ params \ \textbackslash \\ 
		device="/dev/drbd0" \ directory="/opt/zimbra" \ fstype=ext4 \ \textbackslash \\
		op \ start \ interval=0 \ timeout=60s \ \textbackslash \\
		op \ stop \ interval="0" \ timeout="60"
\end{itemize}

\begin{itemize}
	\item Group all resources in the same host:\\
		group zcsgroup zcs\_fs zcs\_service \ \textbackslash \\
		configure colocation VIP1-with-drbd\_ms-Master \ inf: drbd\_ms:Master VIP1 \\
		configure colocation drbd\_ms-Master-with-zcs\_fs \ inf: zcs\_fs drbd\_ms:Master \\
		configure colocation zcs\_fs-with-zcs\_service \ inf: zcs\_service zcs\_fs
\end{itemize}

\begin{itemize}
	\item Order resources:\\
		configure order drbd\_ms-promote-on-VIP1 \ inf: VIP1:start drbd\_ms:promote\\
		configure order zcs\_fs-on-dbrb\_ms-promote \ inf: dbrb\_ms:promote zcs\_fs:start\\
		configure order zcs\_service-on-zcs\_fs \ inf: zcs\_fs:start zcs\_service:start
\end{itemize}

\begin{itemize}
	\item Commit configuration changes and quit:\\
		commit\\
		quit
\end{itemize}

\noindent On both nodes make sure chkconfig is off on every service but DRBD. This means the service will not start up on when the server starts up.\\
	\indent chkconfig corosync off\\
	\indent chkconfig cman off\\
	\indent chkconfig ricci off\\
	\indent chkconfig pacemaker off\\
	\indent chkconfig drbd on

\section{Control and check services}
\label{sec:control}


\begin{itemize}
	\item Check Pacemaker cluster status:\\
		crm\_mon -1\\
		pcs status
\end{itemize}

\begin{itemize}
	\item Check resources status:\\
		crm resource status RESOURCE
\end{itemize}

\begin{itemize}
	\item Check configuration validity:\\
		crm\_verify -L -V
\end{itemize}

\begin{itemize}
	\item Edit values already configured:\\
		crm configure edit\\
	After save changes through the preferred text editor, exit and execute:\\
		cibadmin \textminus \textminus replace
\end{itemize}

\begin{itemize}
	\item Delete existent resource:\\
		pcs resource delete RESOURCE
\end{itemize}

\begin{itemize}
	\item Clean resource history errors (check configuration health):\\
		crm\_resource -P
\end{itemize}

\begin{itemize}
	\item List available classes and resources:\\
		crm ra classes\\
		crm ra list ocf btactic\\
		crm ra list lsb
\end{itemize}

\begin{itemize}
	\item Delete cluster configuration (WARNING):\\
		pcs cluster destroy
\end{itemize}

\section{Testing failover}
\label{sec:failover}


\begin{itemize}
	\item On primary node:\\
		crm node standby\\
	Or stop pacemaker:\\
		service pacemaker stop
\end{itemize}


\begin{itemize}
	\item Now ``crm\_mon" or ``pcs status" will show:\\
		Node astapor.got.com: standby\\
		Online: [ braavos.got.com ]
\end{itemize}


\begin{itemize}
	\item It is going to take a while before secondary node takes control. So it is possible to check logs and ``crm\_mon" status during the process.\\
		crm\_mon\\
		tail -F /var/log/zimbra.log\\
		tail -F /var/log/messages
\end{itemize}


\begin{itemize}
	\item Also it is possible to check with ``crm\_standby" command. A value of true\textbar on indicates that the node is not able to host any resources and a value of false\textbar off indicates it does.\\
		crm\_standby \textminus \textminus get-value
\end{itemize}


\begin{itemize}
	\item At any moment it will be displayed a message like the depicted in Table~\ref{table:failover}.
\end{itemize}

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r l | }
    \hline
      Master/Slave Set: 		& 	drbd\_ms [drbd]\\
      Masters: 					& [ braavos.got.com ]\\
      Slaves: 					& [ astapor.got.com ]\\
      Resource Group: 			& zcsgroup\\
      zcs\_fs (ocf::heartbeat:Filesystem): & Started braavos.got.com\\
      zcs\_service (ocf::btactic:zimbra):  & Started braavos.got.com\\
      VIP1 (ocf::heartbeat:IPaddr2):       & Started braavos.got.com\\
    \hline
  \end{tabular}
\caption{DRBD failover test}
\label{table:failover}
\end{table}

\begin{itemize}
	\item Now the secondary node has control of the cluster resources, while the primary node is in standby or unreachable state. If primary node is back online, secondary node will keep the control of resources, until an explicit node move is done.
\end{itemize}


\begin{itemize}
	\item Set back online the primary node:\\
			crm node online\\
		  Or start over pacemaker service:\\
		  	service pacemaker start
\end{itemize}


\begin{itemize}
	\item To give the control back to primary node, execute on secondary node:\\
			crm node standby\\
		  Then resources will be transferred back to primary node.
\end{itemize}


\begin{itemize}
	\item Finally ``crm\_mon" or ``pcs status" on each node will display a similar to the one showed in Table~\ref{table:sync}
\end{itemize}

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r l | }
    \hline
      Online:                   & [ astapor.got.com braavos.got.com ]\\
      Master/Slave Set: 		& 	drbd\_ms [drbd]\\
      Masters: 					& [ astapor.got.com ]\\
      Slaves: 					& [ braavos.got.com ]\\
      Resource Group: 			& zcsgroup\\
      zcs\_fs (ocf::heartbeat:Filesystem): & Started astapor.got.com\\
      zcs\_service (ocf::btactic:zimbra):  & Started astapor.got.com\\
      VIP1 (ocf::heartbeat:IPaddr2):       & Started astapor.got.com\\
    \hline
  \end{tabular}
\caption{DRBD synced status}
\label{table:sync}
\end{table}


%=====================================
% RESULTS AND DISCUSSION
%
\chapter{Results and discussion}
\label{chap:results}

This chapter presents the evidence and/or results of primary research which you have undertaken

\noindent Presented in the form of:\\

\noindent - hypothesis testing to analysis suggested by guidelines for the adoption of F$\ell$OSS within SMEs (Daffara), search method by collecting and read as much information related to the project is available and select the appropriate solution from a matching set that fulfill the requirements\\

\noindent - Detailed quantitative model OpenBRR, quick and flexible assessment over software tools considered\\

\noindent - Lazy User Model, frame the process by which are chosen the technological tools that will make up the solution that meets user requirements. This model focuses on user needs and the demanded effort when selecting a solution to a problem from a set of possible solutions


%=====================================
% CONCLUSIONS
%
\chapter{Conclusions and future work}
\label{chap:conclusions}

Text..







%=====================================
% BIBLIOGRAPHY
%
\bibliography{ref}{}
\bibliographystyle{plain}


%=====================================
% APPENDIX
%
\appendix
\chapter{btactic zimbra script}
\label{app:appendix1}
% btactic zimbra script

\#!/bin/sh\\
\#\\
\# Resource script for Zimbra\\
\#\\
\# Description:  Manages Zimbra as an OCF resource in an high-availability setup.\\
\#\\
\# Author:       Adrian Gibanel\\
\# \textless adrian.gibanel@btactic.com\textgreater : Original Author\\
\#\\
\# License: GNU General Public License (GPL)\\
\# Note:  Aimed at an active/passive cluster originally\\
\#        Inspired from postfix OCF script\\
\#        Inspired from Ubuntu LSB script.\\
\#        Not sure it will work for other distros without modifying\\
\#\\
\#   usage: \$0 \{start\textbar stop\textbar reload\textbar status\textbar monitor\textbar validate-all\textbar meta-data\}\\
\#\\
\#       The ``start" arg starts Zimbra\\
\#       The ``stop" arg stops it.\\
\#\\
\# OCF parameters:\\
\#  OCF\_RESKEY\_binary\\
\#  OCF\_RESKEY\_config\_dir\\
\#  OCF\_RESKEY\_parameters\\
\#\\
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\\

\noindent \# Initialization:\\

\noindent : \$\{OCF\_FUNCTIONS\_DIR=\$\{OCF\_ROOT\}/lib/heartbeat\}\\
. \$\{OCF\_FUNCTIONS\_DIR\}/ocf-shellfuncs\\
: \$\{OCF\_RESKEY\_binary=``zmcontrol"\}\\
: \$\{OCF\_RESKEY\_zimbra\_dir=``/opt/zimbra"\}\\
: \$\{OCF\_RESKEY\_zimbra\_user=``zimbra"\}\\
: \$\{OCF\_RESKEY\_zimbra\_group=``zimbra"\}\\
USAGE=``Usage: \$0 \{start\textbar stop\textbar reload\textbar status\textbar monitor\textbar validate-all\textbar meta-data\}";\\

\noindent \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\\

\noindent usage() \{\\
    \indent echo \$USAGE \textgreater\&2\\
\}\\

\noindent meta\_data() \{\\

        cat \textless \textless END\\
        
\textless ?xml version=``1.0"?\textgreater\\
\textless !DOCTYPE resource-agent SYSTEM ``ra-api-1.dtd"\textgreater\\
\textless resource-agent name=``zimbra"\textgreater\\
\textless version\textgreater0.1\textless /version\textgreater\\
\textless longdesc lang=``en"\textgreater\\
This script manages Zimbra as an OCF resource in a high-availability setup.\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater\\
Manages a highly available Zimbra mail server instance\\
\textless /shortdesc\textgreater\\

\noindent \textless parameters\textgreater\\

\noindent \textless parameter name=``binary" unique=``0" required=``0"\textgreater\\
\textless longdesc lang=``en"\textgreater\\
Short name to the Zimbra control script.\\
For example, ``zmcontrol".\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater\\
Short name to the Zimbra control script\textless /shortdesc\textgreater\\
\textless content type=``string" default=``zmcontrol" /\textgreater\\
\textless /parameter\textgreater\\

\noindent \textless parameter name=``zimbra\_dir" unique=``1" required=``0"\textgreater\\
\textless longdesc lang=``en"\textgreater\\
Full path to Zimbra directory.\\
For example, ``/opt/zimbra".\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater\\
Full path to Zimbra directory\textless /shortdesc\textgreater\\
\textless content type=``string" default=``/opt/zimbra" /\textgreater\\
\textless /parameter\textgreater\\

\noindent \textless parameter name=``zimbra\_user" unique=``1" required=``0"\textgreater\\
\textless longdesc lang=``en"\textgreater\\
Zimbra username.\\
For example, ``zimbra".\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater Zimbra username\textless /shortdesc\textgreater\\
\textless content type=``string" default=``zimbra" /\textgreater\\
\textless /parameter\textgreater\\

\noindent \textless parameter name=``zimbra\_group" unique=``1" required=``0"\textgreater\\
\textless longdesc lang=``en"\textgreater\\
Zimbra group.\\
For example, ``zimbra".\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater Zimbra group\textless /shortdesc\textgreater\\
\textless content type=``string" default=``zimbra" /\textgreater\\
\textless /parameter\textgreater\\

\noindent \textless /parameters\textgreater\\

\noindent \textless actions\textgreater\\
\textless action name=``start"   timeout=``360s" /\textgreater\\
\textless action name=``stop"    timeout=``360s" /\textgreater\\
\textless action name=``restart"  timeout=``360s" /\textgreater\\
\textless action name=``monitor" depth=``0"  timeout=``40s" interval=``60s" /\textgreater\\
\textless action name=``validate-all"  timeout=``360s" /\textgreater\\
\textless action name=``meta-data"  timeout=``5s" /\textgreater\\
\textless /actions\textgreater\\
\textless /resource-agent\textgreater\\

\noindent END\\
\noindent \}\\

\noindent command()\\
\{\\
  \indent if [ -f \$\{zimbra\_dir\}/redolog/redo.log ]; then\\
  \indent \indent chown -f \$\{zimbra\_user\}:\$\{zimbra\_group\} \$\{zimbra\_dir\}/redolog/redo.log\\
  \indent fi\\
  \indent su - \$\{zimbra\_user\} -c ``\$\{binary\} \$1 \textless /dev/null"\\
\}\\

\noindent running() \{\\
\indent \# run Zimbra status\\
\indent  command status\\
\}\\


\noindent zimbra\_status()\\
\{\\
    \indent running\\
\}\\

\noindent zimbra\_start()\\
\{\\
    \indent \# if Zimbra is running return success\\
    \indent if zimbra\_status; then\\
    \indent \indent ocf\_log info ``Zimbra is already running."\\
    \indent \indent return \$OCF\_SUCCESS\\
    \indent fi\\

    \indent \# start Zimbra\\
    \indent command start\\
    \indent ret=\$?\\
    \indent if [ -d /var/lock/subsys -a \$ret -eq 0 ]; then\\
    \indent \indent touch /var/lock/subsys/zimbra\\
    \indent fi\\

    \indent if [ \$ret -ne 0 ]; then\\
        \indent \indent ocf\_log err ``Zimbra returned an error." \$ret\\
        \indent \indent return \$OCF\_ERR\_GENERIC\\
    \indent fi\\

    \indent \# grant some time for startup/forking the sub processes\\
    \indent sleep 2\\

    \indent \# initial monitoring action\\
    \indent running\\
    \indent ret=\$?\\
    \indent if [ \$ret -ne \$OCF\_SUCCESS ]; then\\
     \indent \indent ocf\_log err ``Zimbra failed initial monitor action." \$ret\\
     \indent \indent return \$OCF\_ERR\_GENERIC\\
    \indent fi\\

    \indent ocf\_log info ``Zimbra started."\\
    \indent return \$OCF\_SUCCESS\\
\}\\


\noindent zimbra\_stop()\\
\{\\
    \indent \# if Zimbra is not running return success\\
    \indent if ! zimbra\_status; then\\
        \indent \indent ocf\_log info ``Zimbra already stopped."\\
        \indent \indent return \$OCF\_SUCCESS\\
    \indent fi\\

    \indent \# stop Zimbra\\
    \indent command stop \\
    \indent ret=\$?\\

    \indent if [ -d /var/lock/subsys -a \$ret -eq 0 ]; then\\
      \indent \indent rm -f /var/lock/subsys/zimbra\\
    \indent fi\\

    \indent if [ \$ret -ne 0 ]; then\\
        \indent \indent ocf\_log err ``Zimbra returned an error while stopping." \$ret\\
        \indent \indent return \$OCF\_ERR\_GENERIC\\
    \indent fi\\

    \indent \# grant some time for shutdown and recheck 5 times\\
    \indent for i in 1 2 3 4 5; do\\
        \indent \indent if zimbra\_status; then\\
            \indent \indent \indent sleep 1\\
        \indent \indent fi\\
    \indent done\\

    \indent \# escalate to abort if we did not stop by now\\
    \indent if zimbra\_status; then\\
        \indent \indent ocf\_log err ``Zimbra failed to stop. Escalating to `abort'."\\

        \indent \indent ORPHANED=\textasciigrave ps -u \$\{zimbra\_user\} -o ``pid="` \&\& kill -9 \$ORPHANED 2\textgreater\&1\\
        \indent \indent ret=\$?\\
        \indent \indent sleep 10\\

        \indent \indent \# zimbra abort did not succeed\\
        \indent \indent if zimbra\_status; then\\
            \indent \indent \indent ocf\_log err ``Zimbra failed to abort."\\
            \indent \indent \indent return \$OCF\_ERR\_GENERIC\\
        \indent \indent fi\\
    \indent fi\\

    \indent ocf\_log info ``Zimbra stopped."\\
    \indent return \$OCF\_SUCCESS\\
\}\\

\noindent zimbra\_restart()\\
\{\\
    \indent if zimbra\_status; then\\
        \indent \indent ocf\_log info ``Reloading Zimbra."\\
        \indent \indent command restart\\
    \indent fi\\
\}\\

\noindent zimbra\_monitor()\\
\{\\
    \indent if zimbra\_status; then\\
        \indent \indent return \$OCF\_SUCCESS\\
    \indent fi\\
    \indent return \$OCF\_NOT\_RUNNING\\
\}\\

\noindent zimbra\_validate\_all()\\
\{\\
    \indent \# check zimbra\_dir parameter\\
    \indent if [ ! -d ``\$zimbra\_dir" ]; then\\
      \indent \indent ocf\_log err ``Zimbra directory `\$config\_dir' does not exist." \$ret\\
      \indent \indent return \$OCF\_ERR\_INSTALLED\\
    \indent fi\\
    \indent \# check that the Zimbra binaries exist and can be executed\\
    \indent if ! have\_binary ``\$\{zimbra\_dir\}/bin/\$\{binary\}" ; then\\
      \indent \indent return \$OCF\_ERR\_INSTALLED\\
    \indent fi\\

    \indent \# check permissions\\
    \indent user=\$\{zimbra\_user\}\\
    \indent zimbra\_writable\_dirs=``\$\{zimbra\_dir\}/conf"\\
    \indent for dir in ``\$zimbra\_writable\_dirs"; do\\
        \indent \indent if ! su -s /bin/sh - \$user -c ``test -w \$dir"; then\\
            \indent \indent \indent ocf\_log err ``Directory `\$dir' is not writable by user `\$user'."\\
            \indent \indent \indent exit \$OCF\_ERR\_PERM;\\
        \indent \indent fi\\
    \indent done\\

    \indent return \$OCF\_SUCCESS\\
\}\\

\noindent \#\\
\# Main\\
\#\\

\noindent if [ \$\# -ne 1 ]; then\\
    \indent usage\\
    \indent exit \$OCF\_ERR\_ARGS\\
fi\\

\noindent binary=\$OCF\_RESKEY\_binary\\
zimbra\_dir=\$OCF\_RESKEY\_zimbra\_dir\\
zimbra\_user=\$OCF\_RESKEY\_zimbra\_user\\
zimbra\_group=\$OCF\_RESKEY\_zimbra\_group\\
parameters=\$OCF\_RESKEY\_parameters\\

\noindent \# build Zimbra options string *outside* to access from each method\\
OPTIONS=`'\\
OPTION\_CONFIG\_DIR=`'\\

\noindent \# check if the Zimbra config\_dir exist\\
if [ ``x\$config\_dir" != ``x" ]; then\\
    \indent \# check for postconf binary\\
    \indent \#check\_binary ``\$\{zimbra\_dir\}/bin/\$\{binary\}"\\

    \# remove all trailing slashes\\
    \indent zimbra\_dir=\textasciigrave echo \$zimbra\_dir \textbar \ sed `s/\/*\$//'\textasciigrave \\
fi\\

\noindent case \$1 in\\
    \indent meta-data)  meta\_data\\
                \indent \indent \indent \indent exit \$OCF\_SUCCESS\\
                \indent \indent \indent \indent ;;\\

    \indent usage\textbar help) usage\\
                \indent \indent \indent \indent exit \$OCF\_SUCCESS\\
                \indent \indent \indent \indent ;;\\
\noindent esac\\

\noindent zimbra\_validate\_all\\
ret=\$?\\

\noindent LSB\_STATUS\_STOPPED=3\\
if [ \$ret -ne \$OCF\_SUCCESS ]; then\\
    \indent case \$1 in\\
    \indent \indent stop)       exit \$OCF\_SUCCESS ;;\\
    \indent \indent monitor)    exit \$OCF\_NOT\_RUNNING;;\\
    \indent \indent status)     exit \$LSB\_STATUS\_STOPPED;;\\
    \indent \indent *)          exit \$ret;;\\
    \indent esac\\
fi\\

\noindent case \$1 in\\
    \indent monitor)    zimbra\_monitor\\
                \indent \indent exit \$?\\
                \indent \indent ;;\\
    \indent start)      zimbra\_start\\
                \indent \indent exit \$?\\
                \indent \indent ;;\\

    \indent stop)       zimbra\_stop\\
                \indent \indent exit \$?\\
                \indent \indent ;;\\

    \indent restart)     zimbra\_restart\\
                \indent \indent exit \$?\\
                \indent \indent ;;\\

    \indent status)     if zimbra\_status; then\\
                    \indent \indent \indent \indent ocf\_log info ``Zimbra is running."\\
                    \indent \indent \indent \indent exit \$OCF\_SUCCESS\\
                \indent \indent \indent else\\
                    \indent \indent \indent \indent ocf\_log info ``Zimbra is stopped."\\
                    \indent \indent \indent \indent exit \$OCF\_NOT\_RUNNING\\
                \indent \indent \indent fi\\
                \indent \indent \indent ;;\\

    \indent validate-all)   exit \$OCF\_SUCCESS;;\\

    \indent *)          usage\\
                \indent \indent exit \$OCF\_ERR\_UNIMPLEMENTED;;\\
esac

\end{document}