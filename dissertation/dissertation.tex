\documentclass[a4paper, 12pt]{book}
\usepackage[a4paper, left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}
\usepackage{array}
\usepackage[english]{babel}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{float}  									 % H for Figures positioning
\usepackage{graphicx}
\usepackage[latin1]{inputenc}
\usepackage{latexsym}  									 % LaTeX Logo
\usepackage{placeins}
\usepackage{textcomp}
\usepackage{times}
\usepackage{titlesec}
\usepackage[nottoc, notlot, notlof, notindex]{tocbibind} % Index options
\usepackage{url}

\pagestyle{plain}
\graphicspath{{./img/}}

\title{Implementation of a high availability solution based on Free Libre Open Source Software tools for Netnovation's Email and Collaboration System}
\author{Daniel H. G\'{a}mez V.}

\renewcommand{\baselinestretch}{1}  					% Interlining

\begin{document}

%=====================================
% COVER
%
\begin{titlepage}
  \begin{center}
  \begin{tabular}[c]{c c}
    \includegraphics[scale=0.25]{logo_vect.eps}
    \begin{tabular}[b]{l}
      \Huge
      \textsf{UNIVERSIDAD} \\
      \Huge
      \textsf{REY JUAN CARLOS} \\
    \end{tabular}
  \end{tabular}
  \vspace{3cm}
  
  \Large
  Master in Free Libre Open Source Software\\
  \vspace{0.2cm}
  \large
  Academic Course 2014/2015 \\
  \vspace{0.4cm}
  Master Thesis \\
  \vspace{1cm}
  \LARGE
  Implementation of a high availability solution based on Free Libre Open Source Software tools for Netnovation's Email and Collaboration System \\
  \vspace{2cm}
  \large
  Author: DANIEL H. G\'{A}MEZ V.\\
  Tutor: DR. GREGORIO ROBLES
  
  \end{center}
\end{titlepage}

%=====================================
% LICENSE
%
{\raggedleft
(c) 2015, Daniel H. G\'{a}mez V.\\
 daniel.gamez@gmail.com\\
 This work is licensed under a\\
 Creative Commons Attributions 3.0 License
  \begin{figure}[H]
    {\raggedleft
    \includegraphics[scale=0.80]{by-sa.png}\\
    \hfill http://creativecommons.org/licenses/by-sa/3.0/legalcode
    }
  \label{fig:logo}
  \end{figure}
}

%=====================================
% ABSTRACT
%
% \begin{abstract} This is the Abstract... \end{abstract}
% \chapter*{\centering Abstract}
\chapter*{Abstract}
\label{chap:abstract}
\addcontentsline{toc}{chapter}{Abstract}

This is the Abstract...

\noindent
Key words: Cluster, Corosync, CRM, DRBD, FOSS, F$\ell$OSS, High Availability, OCF, Pacemaker, Zimbra

%=====================================
% ACKNOWLEDGEMENTS
%
\chapter*{Acknowledgements}
\label{chap:acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}

\noindent A huge recognition to the Free Libre Open Source Software and its wonderful community, thanks to the many opportunities and satisfactions it has given to me.\bigskip

\noindent To the Universidad Rey Juan Carlos and its prestigious team of professors and academics, in particular to the Grupo de Sistemas y Comunicaciones (GSyC) and Libresoft who promote the movement of Free Libre Open Source Software with such passion and excellent quality.\bigskip

\noindent Also to the Netnovation team and especially Eduardo V\'{i}tols, who has supported me for a long time. This project has been possible thanks to their innovation spirit.\bigskip

\noindent To my beautiful country Venezuela, and also to this graceful land in which I reside, Spain. To my family and friends, who encourage me everyday to go forward.


%=====================================
% TERMINOLOGY
%
\chapter*{Terminology}
\label{chap:terminology}
\addcontentsline{toc}{chapter}{Terminology}

\begin{itemize}[label={}]
	\item API: Application Programming Interface
	\item CLI: Command Line Interface
	\item cLVM: clustered Logical Volume Manager
	\item CPU: Central Process Unit
	\item CRM: Cluster Resource Manager / Customer Relationship Management
	\item CTDB: Clustered Trivial Database
	\item DRBD: Distributed Replicated Block Device
	\item F$\ell$OSS: Free Libre Open Source Software
	\item GNU: GNU is Not Unix
	\item GPL:  General Public License
	\item GUI: Graphic User Interface
	\item HA: High Availability
	\item HAWK: HA Web Konsole
	\item HDD: Hard Disk Drive
	\item IPv4: Internet Protocol version 4
	\item ISO: International Organization for Standardization
	\item IT: Information Technologies
	\item KVM: Kernel-based Virtual Machine
	\item LGPL: Lesser General Public License
	\item LSB: Linux Standard Base
	\item OBS: openSUSE Build Service
	\item OCF: Open Cluster Framework
	\item OCFS: Oracle Cluster File System
	\item OS: Operating System
	\item PBX: Private Branch Exchange
	\item RHEL: Ret Hat Enterprise Linux
	\item SAN: Storage Area Network
	\item SCSI: Small Computer System Interface
	\item SLA: Service Level Agreement
	\item SME: Small and Medium Enterprises
	\item VPN: Virtual Private Network
	\item VPS: Virtual Private Server
	\item WAN: Wide Area Network
	\item YaST: Yet another Setup Tool
	\item ZCS: Zimbra Collaboration System
\end{itemize}


%=====================================
% CONTENTS
%
\tableofcontents  	% General Index
\listoffigures  	% Figures Index
\listoftables 		% Tables Index

%=====================================
% INTRODUCTION
%
\chapter{Introduction}
\label{chap:introduction}

\noindent With the growing use of cloud-oriented systems and the need for this information to be always available, online systems play an increasingly important role in our society nowadays. The technologies that support such schemes have evolved fleetingly and today there are numerous ways to get this kind of solutions, from proprietary software implementations, tools oriented to corporate environments that can be even based on F$\ell$OSS products and a variety of business models, to F$\ell$OSS standalone tools representing a robust solution to meet today's demands.\bigskip

\noindent Since 2004 Netnovation\texttrademark \ is a Venezuelan SME formed by a team of professionals in the areas of IT and telecommunications who adopted a business model based on consulting around F$\ell$OSS, providing system integration, timely development and Software as a Service (SaaS) cloud services.\bigskip

\noindent Due to a number of reasons that will be discussed throughout this dissertation, cloud services require a set of components to ensure the security, availability and reliability of data that is stored in data center facilities remotely accessible from internet. The study conducted here is focused specifically on the availability of the data that must be accessible to their applicants at all times.\bigskip

\noindent This study was undertaken in the one hand to test some aspects of business practices in the current technology market and the use of F$\ell$OSS as a key factor, and in the other one, to fulfill the demands of a business organization showing that business models around F$\ell$OSS are a fact.\bigskip

\noindent The way this dissertation is going to be organized is the following:\bigskip

\noindent Chapter 2 with the problem statement, setting why there is an issue in the current situation, what are the justification and motivation of this dissertation, also defining the objectives and the proper scope.\bigskip

\noindent Chapter 3 describing the most relevant related technologies around the possible solutions.\bigskip

\noindent Chapter 4 establishing the used methodology, supported by guidelines proposed by Carlo Daffara and the Lazy User model, as well as analysis of concrete metrics.\bigskip

\noindent Chapter 5 showing the architecture of the company and its infrastructure, available hardware for operations, the network scheme, and an overview of the software supporting the current situation, all of this in order to understand how to adapt an actual solution.\bigskip

\noindent Chapter 6 describing the implemented technologies to achieve the solution for the problem stated.\bigskip

\noindent Chapter 7 will provide a detailed description of the actual implementation for the proposed solution.\bigskip

\noindent Chapter 8 with the results and discussion around the subject.\bigskip

\noindent Finally in Chapter 9 the conclusions and future work will be raised.\bigskip


%=====================================
% PROBLEM STATEMENT 
%
\chapter{Problem statement}
\label{chap:problem}

Business continuity in the field of information technology is supported in a large extent by the uninterrupted operation of the systems used in productivity tasks~\cite{ISO22399}. These systems must be fault tolerant, so that operations have the least possible impact in the event that an unexpected incident occurs.\bigskip

\noindent Nowadays there are increasingly more people and organizations using centralized remote systems that allow online access to resources and everyday services, this scheme is called cloud computing~\cite{MandG}. Through this type of services, end users whether individuals or corporations, are abstracted to support the infrastructure that this entails, giving responsibility to intermediary companies providing cloud services. So these intermediaries are the ones who must ensure the proper availability of the services, as well as factors such as communications security and redundancy of stored data, among many others.\bigskip

\noindent In particular Netnovation is a SME in the field of information technologies, which provides private cloud services from data storage to hosting virtual private servers (VPS), including e-mail and collaboration servers. The latter is precisely one of the mainstays for the operations of the company, which employs mainly F$\ell$OSS to its internal systems, specifically using the F$\ell$OSS e-mail and collaboration suite Zimbra\texttrademark\footnote{\ http://www.zimbra.com}. One of the main problems that Netnovation faces is to ensure the communication and workflow continuity that is carried through this collaboration tool, as well as meet the SLAs offered to its customers over this software.\bigskip

\noindent There are various software solutions offering high availability of services such as those provided by Zimbra, each one with its own legal implications, associated costs and implementation difficulty\footnote{\ Some of these solutions will be addressed in Chapter~\ref{chap:related}}. A valid alternative is the integration of multiple tools in the field of F$\ell$OSS providing a framework to ensure  continuity of systems operation or business continuity. By doing it this way it is possible to adapt the different requirements and use different technologies to provide the most consistent solution to what is desired.\bigskip

\noindent One key technology for this purpose is a cluster, in particular a high availability cluster, basically a a group of computers interconnected that work together trying to maintain a service up and running all the time.\bigskip

\noindent On previous occasions, Netnovation has managed to successfully consolidate most of its operations infrastructure adapting F$\ell$OSS, making it a wonderful idea to keep this scheme working. To achieve this, it is necessary to evaluate the state of the art in the field of systems that provide high availability, with the aim of offering an effective solution, all of this in accordance to the guidelines that have been proposed by the company.


\section{Justification / Motivation}
\label{sec:justification}

The factors that motivate this work are on one hand, give proper credit to business models based on F$\ell$OSS as those used by technology companies nowadays~\cite{Daffara2}, and on the other hand, show that private enterprise can be benefited by F$\ell$OSS  through a set of toolsets and mechanisms which allow obtaining robust solutions in accordance to technology needs.

\section{Objectives}
\label{sec:objectives}

The overall objectives are:

\begin{itemize}
	\item To frame the F$\ell$OSS business model used by Netnovation
	\item To show various current alternatives provided by F$\ell$OSS at the corporate level
	\item To adapt the proposed solution to the guidelines established by Netnovation
	\item To establish an initial point reference for implementing high availability private cloud services offered by Netnovation
\end{itemize}

\noindent The specific objectives are:

\begin{itemize}
	\item To implement a high availability solution based on F$\ell$OSS for the e-mail and collaboration system Zimbra used by Netnovation
	\item To describe the methodology used for the selection of the solution to be implemented
	\item To describe the process undertaken to implement the selected solution
	\item To perform tests in a controlled laboratory environment and validate the correct operation of the solution in order to promote it to a production environment
\end{itemize}


\section{Scope}
\label{sec:scope}

The solution to be implemented consists of F$\ell$OSS tools that allow its adaptation to the current infrastructure of Netnovation, they are not intended to replace the elements of the existing operations  platform.\bigskip

\noindent The methodology used for the selection of F$\ell$OSS tools that make the proposed solution is not intended to provide an exhaustive process that considers all possibilities in the area, but a flexible way that allows classify them qualitatively, justifying their choice through concrete metrics.\bigskip

\noindent Having successfully implemented a high availability solution on the e-mail and collaboration system used by Netnovation, this will serve as a reference for providing high availability to other enterprise systems, but these other configurations are not covered in this exercise.\bigskip


%=====================================
% PRECEDENTS / RELEATED TECHNOLOGIES / STATE OF THE ART
%
\chapter{Related technologies}
\label{chap:related}
A possible way to categorize high-availability technologies are Enterprise Solutions and F$\ell$OSS based tools, considering that usually the first ones have associated support plans over the whole provided solution, whereas the second ones provide support over its own tools, but not necessarily over the whole cluster implementation.

\section{Commercial Enterprise Cluster Software}
\label{sec:enterprise}

Some of these high-availability implementations have been proprietary since the beginning, either way at some point commercial companies realized that these technologies could be integrated into open systems thanks to licenses such as GNU/LGPL. Since then, as a business model strategy, companies such as Hewlett Packard, Red Hat Inc. and SUSE (a Novell company) provide support plans over open system platforms, considering their entire solution and charging for business licenses, generally on annual basis. In the following sections there is a description for each of them.

\subsection{HP Serviceguard}
\label{subsec:serviceguard}

Hewlett Packard (HP) claims the credit for the development of the first high availability solution for UNIX systems since 1990\footnote{\url{http://www.hpintelco.net/sglx/service.html}}. MC/ServiceGuard is a high-availability cluster software released for HP-UX and later for GNU/Linux systems. Since the first development of the software, HP has partnered with with companies such as Oracle or SAP to deploy high-availability in enterprise environments, as Figure~\ref{fig:enterprise} (took from \url{http://www.hpintelco.net}) shows.\bigskip

\begin{figure}
  \centering
  \includegraphics[scale=0.70]{commercial-ha.png}
  \caption[High-availability race in enterprise environments]{High-availability race in enterprise environments}
  \label{fig:enterprise}
\end{figure}

\noindent With the appearance of Linux in 1994 and its increasing popularity together with GNU, in 1999 HP released a Linux port called SG/LX, allowing high-availability features on it.\bigskip

\noindent Since 2001, companies Intel, Red Hat and HP joined efforts to produce the Red Hat Open Source Solutions Initiative\footnote{\url{http://www.hpintelco.net/hp-intel-redhat.htm}} (OSSI), with the aim to reduce partner's sale cycle by delivering enterprise reliable solutions to their customers. To some extent, realizing that their technologies could be strengthened by F$\ell$OSS.\bigskip

\noindent Some technical specifications of this product are the following:

\begin{itemize}
	\item Proprietary Licensing Model\footnote{http://h20564.www2.hp.com/hpsc/doc/public/display?docId=emr\_na-c02199685}
	\item Active/active, active/standby, and rotating standby cluster types
	\item Quorum Server support
	\item 32 Fibre Channel nodes, 2 single-path SCSI nodes, and 32 multipath SCSI nodes
	\item Operating Systems RHEL and SLES
	\item Hardware HP ProLiant ML, DL, and BL G7, Gen8 and Gen9 servers
	\item HP 3PAR, EVA, StoreSure, EMC VMAX, VNX storage
	\item ext3, ext4, NFS, XFS, VxFS, VxVM and btrfs filesystems
	\item Logical Volume Manager
\end{itemize}

\noindent The general overview of an HP Serviceguard architecture is shown in Figure~\ref{fig:serviceguard} (took from data sheet HP Serviceguard Solutions for Linux\footnote{http://h20195.www2.hp.com/v2/GetDocument.aspx?docname=4AA4-1792ENW}).

\begin{figure}
  \centering
  \includegraphics[scale=0.70]{hp-serviceguard.png}
  \caption[Basic HP Serviceguard for Linux cluster]{Basic HP Serviceguard for Linux cluster}
  \label{fig:serviceguard}
\end{figure}


\subsection{Red Hat Cluster Suite}
\label{subsec:rhcs}

At the end of the 90's Red Hat Inc. introduced its Enterprise Linux Advanced Server, designed specifically for use in enterprise environments to deliver superior application support, performance, availability and scalability. It included a high-availability clustering feature as part of the base product. Since then, the product has evolved to the current Red Hat Cluster Suite (RHCS), provided as a separately licensed product, also on top of Red Hat's Linux Enterprise Server.\bigskip

\noindent The RHCS has three major features, one of them is the Cluster Manager or \textbf{cman}, which adds functionality to, and is dependent upon other cluster stacks such as Corosync or OpenAIS. Is noteworthy that this is an adaptation of the Linux-HA project. Another key feature is the Resource Group Manager or \textbf{rgmanager}, a fully functional replacement for Pacemaker intended to work exclusively with RHCS. And also the IP Load Balancing (originally called Piranha), was originally developed by researchers at Oak Ridge National Laboratory, basically a text mining technology that Red Hat adapted in order to allow transparent load balancing and failover between servers\footnote{http://www.ornl.gov/connect-with-ornl/for-industry/partnerships/technology-licensing/licensing-opportunity-announcements/piranha}.\bigskip

\noindent The Cluster Manager, the Resource Group Manager and the IP Load Balancing are complementary high-availability technologies that can be used separately or in combination, depending on application requirements. Some of these technologies come from previous F$\ell$OSS projects and have been properly integrated into RHCS.\bigskip

\noindent Some technical details of this product are the following:

\begin{itemize}
	\item Support for up to 128 nodes (16 nodes on Red Hat Enterprise Linux 3, 4, 5, and 6)
	\item NFS, CIFS, GFS share and cluster filesystem managers
	\item File system and services failover support
	\item Fully shared storage subsystem
	\item Comprehensive data integrity
	\item SCSI and fiber channel support
	\item OCF and LSB resource agents
\end{itemize}

\noindent A general RHCS infrastructure is shown in Figure~\ref{fig:rhcs} (took from RHCS online documentation\footnote{\url{https://access.redhat.com/documentation/en-US/Red\_Hat\_Enterprise\_Linux/5/html/Cluster\_Suite\_Overview/images/9106.png}}).

\begin{figure}
  \centering
  \includegraphics[scale=0.70]{rhcs.png}
  \caption[RHCS Basic Infrastructure]{RHCS Basic Infrastructure}
  \label{fig:rhcs}
\end{figure}


\subsection{SUSE Linux Enterprise High Availability Extension}
\label{subsec:suseha}

This is an integrated suite of clustering technologies that enables the implementation of high availability over physical and virtual Linux clusters~\cite{TandS}. It allows monitoring, messaging, and cluster resource management, handling failover and load balancing of resources.\bigskip

\noindent As Red Hat did, other companies such as Novell took advantage of the high-availability demand over the enterprise sector and also adapted their own HA solution. This product is available as a paid add-on to SUSE Linux Enterprise Server GNU/Linux distribution, although in OpenSUSE many of these tools are included into the base system for free (without charge), with available repositories on OBS to provide newer versions of the packages for various GNU/Linux distributions\footnote{\url{https://en.opensuse.org/openSUSE:High_Availability}}.\bigskip

\noindent Among the main product features of this product are the following:

\begin{itemize}
	\item Multiple clustering scenarios, as active/active and active/passive configurations, as well as hybrid physical and virtual clusters
	\item Supports mixed clustering, physical and virtual Linux servers, based on Xen and KVM hypervisors
	\item Corosync messaging and membership layer, also Pacemaker cluster resource manager
	\item Storage and data replication supporting Fibre Channel or iSCSI SAN
	\item Cluster-aware file systems with GFS and OCFS, and cLVM as volume manager
	\item Supports replication through DRBD
	\item Samba clustering with CTDB
	\item Provides resource agent manager OCF
	\item GUI and CLI administration tools, such as YaST, HAWK and CRM
\end{itemize}

\noindent A SUSE HA cluster architecture is depicted in Figure~\ref{fig:suseha} (took from SUSE Linux High Availability Extension online documentation\footnote{https://www.suse.com/documentation/sle\_ha/book\_sleha/graphics/ha\_cluster\_components\_arch.png}).

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.50]{suse-ha.png}
  \caption[SUSE HA Cluster Components]{SUSE HA Cluster Components}
  \label{fig:suseha}
\end{figure}


\section{High availability F$\ell$OSS based tools}
\label{sec:hafloss}

% *** Include professional support offered by these software vendors ***

Some of these tools are possibly part of the enterprise products mentioned before, but they do not necessarily have an infrastructure providing corporate-oriented services. Either way, some of these F$\ell$OSS technologies are backed by professional support over their standalone software, giving rise to business models such as product specialists~\cite{Daffara1}.\bigskip

\noindent To understand the comprehensive solution that these tools are capable to provide, it is appropriate to understand briefly how an HA cluster works and how its components have evolved since the appearance of this concept.\bigskip

\noindent An HA cluster could be defined as a group of computers supporting server applications, ensuring its accessibility with a minimum of down-time. They operate by using specialized software that leverage the redundancy of computers and avoid single points of failure by implementing a cluster architecture~\cite{Weygant}.\bigskip

\noindent Then, as long as there is a service that must be maintained up and running and accessible to the users, computers require a way to communicate to one another and coordinate between each other to provide this service. Here computers are running GNU/Linux operating system and rely on TCP/IP network protocol, as well as network interconnected hardware to achieve the communication between nodes in the cluster.\bigskip

\noindent One of the earliest tools who managed this task was Heartbeat, as a daemon installed on each node, able to \textit{talk} to the other ones and share cluster related information. This task was referred to as the Cluster Messaging Layer or Group Communication System.\bigskip

\noindent Another important branch evolved, focused on the service or group of resources that the cluster is supposed to provide, for which Heartbeat incorporated a Cluster Resource Manager (CRM). A process to manage software resources, making use of scripts known as \textit{Resource Agents} responsible to perform actions depending on the status of each node.\bigskip

\noindent With these two distinct groups, one mostly concerned with the cluster messaging, and the other concerned with cluster resources, begins an important race of technologies seeking specialization on each area.\bigskip

\noindent Heartbeat, part of the Linux-HA project, was first released in 1999 under GNU GPL and GNU LGPL license, maintaining a set of building blocks for high availability cluster systems, including a cluster messaging layer, a bunch of resource agents for a variety of applications, a plumbing library and an error reporting toolkit. Around 2007 this project evolved to Pacemaker, integrating or allowing interaction with multiple F$\ell$OSS cluster stacks such as Corosync and OCF.\bigskip

\noindent Nowadays Pacemaker is a resource manager responsible for starting and stopping cluster services in a proper way. Combined with other tools is able to detect service-level failures and move resources between cluster nodes as needed, to ensure the smooth operation of the services.\bigskip

\noindent On the other hand Corosync is responsible for cluster membership, message passing and quorum, using the totem protocol 
for heartbeat, monitoring other node's health.\bigskip

\noindent Technologies such as OpenAIS (a software API) also compete into the HA race. It is a F$\ell$OSS implementation of the Application Interface Specification released under the terms of the Artistic License\footnote{ http://opensource.org/licenses/Artistic-2.0}, used to define how HA applications work together, trying to mask hardware, operating system, middleware and application-level failures.\bigskip

\noindent As in enterprise HA solutions, Cluster Managers are an important element within HA implementations, providing backend GUI or CLI software that runs on one or all cluster nodes, responsible for managing and controlling clustered services. Currently there are several F$\ell$OSS alternatives, including the ones listed in Table~\ref{table:cluster_managers}.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r m{10cm} | }

    \hline    
    \multicolumn{2}{|c|}{\textbf{Apache Mesos}}\\
    \hline
    Company: &  Apache Software Foundation\\
    License: & Apache License Version 2.0 \\
    Website: & http://mesos.apache.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{Keepalived}}\\
    \hline
    Company: &  Individual - Alexandre Cassen\\
    License: & GNU GPL v2+ \\
    Website: & http://keepalived.sourceforge.net/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
    \multicolumn{2}{|c|}{\textbf{Linux Cluster Manager}}\\
    \hline
    Company: & Individual - Michael England \\
    License: & GNU GPL \\
    Website: & http://linuxcm.sourceforge.net/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{oneSIS}}\\
    \hline
    Company: &  Individual - Josh England\\
    License: & GNU GPL v2 \\
    Website: & http://onesis.org/ \\
    Description: &  \\
    Commercial support: &  \\

    \hline    
    \multicolumn{2}{|c|}{\textbf{Rocks Cluster Distribution}}\\
    \hline
    Company: & Rocks Cluster Group \\
    License: & BSD 3-Clause like \\
    Website: & http://www.rocksclusters.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
  \end{tabular}
\label{table:cluster_managers_1}
\end{table}

\hfill\break

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r m{10cm} | }
  
    \hline    
    \multicolumn{2}{|c|}{\textbf{SCMS.pro}}\\
    \hline
    Company: & Glushkov Institute of Cybernetics NAS of Ukraine\\
    License: & Apache 2 License \\
    Website: & http://www.scms.pro/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
    \multicolumn{2}{|c|}{\textbf{Ultra Monkey}}\\
    \hline
    Company: & Individual - Simon Horman \\
    License: & LGPL v2 \\
    Website: & http://www.ultramonkey.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{xCAT}}\\
    \hline
    Company: & Individual - Bruce Sawyers, Jarrod Johnson et al.\\
    License: & Eclipse Public License \\
    Website: & http://xcat.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
  \end{tabular}
\caption{F$\ell$OSS Cluster Managers}
\label{table:cluster_managers}
\end{table}


\noindent The cluster oriented filesystems are another key element regarding HA implementations. They provide data replication and fault tolerance, allowing operations continuity against incidents. There are several F$\ell$OSS alternatives to provide clustered storage, including the tools listed in Table~\ref{table:cluster_filesystems}. One of the most consolidated clustered storage technologies is DRBD, first released in 1999, it will be aborded in Chapter~\ref{chap:implemented}.

%\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r m{10cm} | }

    \hline
    \multicolumn{2}{|c|}{\textbf{Apache Hadoop}}\\
    \hline
    Company: & Apache Software Foundation \\
    License: & Apache License 2 \\
    Website: & http://hadoop.apache.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{GFS}}\\
    \hline
    Company: & Red Hat Inc. \\
    License: & GPL \\
    Website: & http://sourceware.org/cluster/gfs/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
  \end{tabular}
\label{table:cluster_filesystems_1}
\end{table}

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r m{10cm} | }

    \hline
    \multicolumn{2}{|c|}{\textbf{GlusterFS}}\\
    \hline
    Company: & Red Hat Inc. \\
    License: & GPL v3 \\
    Website: & http://www.gluster.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline
    \multicolumn{2}{|c|}{\textbf{Lustre FS}}\\
    \hline
    Company: & Seagate Technology LLC \\
    License: & GPL \\
    Website: & http://lustre.org/ \\
    Description: &  \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{MooseFS}}\\
    \hline
    Company: & Core Technology Inc. \\
    License: & GPL v2 \\
    Website: & https://moosefs.com/ \\
    Description: &  \\
    Commercial support: &  \\

    \hline    
    \multicolumn{2}{|c|}{\textbf{OCFS2}}\\
    \hline
    Company: & Oracle Corp. \\
    License: & GPL \\
    Website: & https://oss.oracle.com/projects/ocfs2/ \\
    Description: & Oracle Cluster File System \\
    Commercial support: &  \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{XtreemFS}}\\
    \hline
    Company: & Quobyte Inc. \\
    License: & BSD License \\
    Website: & http://www.xtreemfs.org/ \\
    Description: &  \\
    Commercial support: &  \\
    \hline    
    
  \end{tabular}
\caption{F$\ell$OSS Cluster Storage Technologies}
\label{table:cluster_filesystems}
\end{table}


%=====================================
% METHODOLOGY
%
\chapter{Methodology}
\label{chap:methodology}

The followed roadmap to achieve the objectives outlined is a set of guidelines and suggestions for the adoption of F$\ell$OSS within SMEs~\cite{Daffara1}, in the sense that a methodology is not an exact formula but a set of practices. At using this model, companies find a supporting guide from the initial selection and adoption of F$\ell$OSS within the IT infrastructure and even to the consolidation of business models around open source.\bigskip

\noindent On the one hand, the guidelines proposed by Daffara suggest a research method by collecting and read as much information related to the project is available and select the appropriate solution from a matching set that fulfill the requirements. On the other hand, following a practical formal approach, applying a model of technology acceptance such as the Lazy User Model (LUM)~\cite{CandT1} is possible to frame the process by which are chosen the technological tools that will make up the solution that meets user requirements, which in this case is being represented by the company Netnovation. This model focuses on user needs and the demanded effort when selecting a solution to a problem from a set of possible solutions, ``According to the lazy user model, a user is likely to choose the solution that requires the least effort. The user examines this cost in terms of time, energy and money when considering how to use a new solution."~\cite{CandT2} The LUM proposes that technology acceptance is impacted 
by this principle.\bigskip

\noindent Additionally at using MetricsGrimoire~\cite{GSyC}, a flexible toolset which allow to obtain data from repositories related to software development, is possible to analyze databases that can later be mined for specific patterns or summaries of activity, allowing to establish objective comparisons in relation to the projects analyzed.

% python, matplotlib ?

%=====================================
% ARCHITECHTURE
%
\chapter{Architecture}
\label{chap:architecture}

In order to provide IT services to customers, considering software services hosted in an on-line remote location, Netnovation requires a proper software and hardware infrastructure to operate. Some of these services are from VPS, Data Storage Systems, Customer Relationship Management Systems (CRM), Email and Collaboration Systems, to Voice over IP PBX systems. Both parties, software infrastructure and product services offered are based on F$\ell$OSS.\bigskip

\noindent In particular the Zimbra server to which a high availability schema is been configured, resides into this architecture, and it is consistent with the company's principles and business model, which is why it is useful to understand the environment to which it belongs.


\section{Company Infrastructure}
\label{sec:infrastructure}

Currently the services are offered from two DataCenters (DC1 and DC2) geographically distributed with the aim of guarantee data redundancy. Assuming that communication with the main DC is lost, it has been defined a procedure that allows the restoration of services in the other DC, with the disadvantage that it is a manual procedure that requires administrators intervention.

\section{Existent Hardware}
\label{sec:hardware}

Each DC has an average of seven Dell PowerEdge\texttrademark \ Racked Servers with different capacities, interconnected via communication devices that provide various services analogously.\bigskip

\noindent There is a Dell PowerEdge 2850/2950 server serving as firewall and main router on each DC. On the one hand it has a WAN 1000Mbps interface, which is connected through UTP Cat-6 wired to 24 PoE ports Switches Netgear FS728TPv1 Gigabit. Physical servers are installed in 20U rack cabinets. These servers range from models Dell PowerEdge 1950, R510 to R710, have Intel Xeon CPUs within 24 and 64 cores, count with 8 to 64GB of RAM, and also have SCSI HDD with capacities between 100GB and 2,5TB.


\section{Company Network Scheme}
\label{sec:networkscheme}

The housing services leased by the providers offer a pool of public IPv4 addresses that are handled by the main router on each DC facility. DC1 and DC2 are interconnected by a VPN through WAN, each of them associated to a different private Class B network internally. To the LAN Ethernet ports of the switches are connected the physical servers of the private network with transfer speed rates of 100/1000Mbps. The overall interconnection scheme can be appreciated in Figure~\ref{fig:network}.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.50]{network_scheme.png}
  \caption[Network interconnection scheme]{Network interconnection scheme}
  \label{fig:network}
\end{figure}


\section{Software Supporting the Infrastructure}
\label{sec:software}

In Table~\ref{table:technologies} there are some software solutions currently used by Netnovation that are related with the required architecture to provide IT cloud-oriented services, with a brief description and legal licensing information for each one of them.

\begin{table}
  \centering
  \begin{tabular}{ | r m{10cm} | }
  
    \hline    
    \multicolumn{2}{|c|}{\textbf{UTM Endian Firewall}}\\
    \hline
    Company: & Endian S.r.l. \\
    Industry: & Unified Threat Management \\
    License: & GNU GPL \\
    Website: & endian.com \\
    Description: & A linux security distribution with full featured Unified Threat Management functionality. Include a stateful packet inspection firewall, application-level proxies for various protocols, antivirus support, virus and spam-filtering for email traffic, content filtering of Web traffic, also an OpenVPN solution. Distribution based on Red Hat. \\
    Supported Platforms: & GNU/Linux \\
    Commercial support: & annual subscription \\
    
    \hline
    \multicolumn{2}{|c|}{\textbf{Proxmox VE}}\\
    \hline
    Company: & Proxmox Server Solutions GmbH \\
    Industry: & Server Virtualization \\
    License: & GNU Affero and GPLv3 \\
    Website: & pve.proxmox.com \\
    Description: & Virtualization management solution for servers, based on KVM and containers Server Virtualization Platform, provides KVM and OpenVZ hypervisors. Distribution based on Debian. \\
    Supported Platforms: & GNU/Linux \\
    Commercial support: & annual subscription \\
    
    \hline    
    \multicolumn{2}{|c|}{\textbf{FreeNAS}}\\
    \hline
    Company: & iXsystems, Inc. \\
    Industry: & Computer Storage \\
    License: & BSD 2-Clause \\
    Website: & freenas.org \\
    Description: & Network-attached storage server, supporting many network and storage protocols such as Samba and NFS. Also supports ZFS. Distribution based on FreeBSD. \\
    Supported Platforms: & BSD Unix \\
    Commercial support: & custom quotes and support tickets \\
    \hline
    \multicolumn{2}{|c|}{\textbf{Zabbix}}\\
    \hline
    Company: & Zabbix SIA \\
    Industry: & IT Monitoring \\
    License: & GNU GPLv2 \\
    Website: & zabbix.com \\
    Description: & Solution for monitoring of networks, applications and databases. \\
    Supported Platforms: & GNU/Linux \\
    Commercial support: & custom quotes and support tickets \\
    \hline
  \end{tabular}
\caption{Software Supporting the Infrastructure}
\label{table:technologies}
\end{table}


%=====================================
% IMPLEMENTED TECHNOLOGIES
%
\chapter{Technological background}
\label{chap:implemented}

The following software tools represent the key elements on which it has been possible to implement a comprehensive high availability solution, some of them mentioned in Section~\ref{sec:hafloss}.

\begin{itemize}
	\item Red Hat Enterprise Linux Server
\end{itemize}

\noindent GNU/Linux enterprise-oriented distribution providing a very stable base system, vast documentation and proper support from manufacturer, released as F$\ell$OSS mainly under the terms of the GNU Lesser General Public License 2.1, except for some optional components. In order to be specific in this exercise, the Linux kernel 2.6.32-431.el6.x86\_64 that is included by the RHEL version 6.5 was used.

\begin{itemize}
	\item Zimbra Collaboration System (ZCS)
\end{itemize}

\noindent Server and client collaboration software, supporting e-mail, contacts, calendar, documents, push synchronization, and many other enterprise features related to groupware. The software is F$\ell$OSS released under the terms of the Common Public Attribution License version 1 and the GNU General Public License version 2 (GPLv2). The exact version implemented was ZCS FOSS  8.0.7\_GA\_6021.RHEL6\_64.

\begin{itemize}
	\item Distributed Replicated Block Device (DRBD)
\end{itemize}

\noindent A distributed replicated storage system for Linux, implemented as several userspace management applications and shell scripts, used to provide data redundancy. Works on top of block devices, such as hard disk partitions or LVM logical volumes, mirroring each data block that it is written to disk to the peer node.

\begin{itemize}
	\item Corosync
\end{itemize}

\noindent It is released as F$\ell$OSS under the 3-clause BSD License. This software provides features based on C programming language implementing high availability within applications, through virtual synchrony for replicated state machines, simple availability handling responsible for applications restart when fail, it keeps configuration and statistics in a memory database providing the ability to set, retrieve, and receive change notifications of information, and a quorum system that notifies applications when it is achieved or lost.

\begin{itemize}
	\item Pacemaker
\end{itemize}

\noindent A F$\ell$OSS high availability resource manager software released under GNU GPLv2. This software was part of the Linux-HA project until 2007, then was split out to be its own project. It implements APIs for resources control, including the Open Cluster Framework (OCF). It is used on computer clusters since 2004.

\begin{itemize}
	\item Cluster Resource Manager Shell (CRMsh\footnote{http://crmsh.github.io}) and Pacemaker Configuration System (PCS\footnote{https://github.com/feist/pcs})
\end{itemize}

\noindent Initially, the CRMsh was distributed as part of the Pacemaker project, but it was split into its own separate project in 2011. Also as CRMsh, PCS is a command-line interface to the Pacemaker cluster resource management stack.

\begin{itemize}
	\item Cluster Configuration System (CCS)
\end{itemize}

\noindent Manages the cluster configuration and provides information to other cluster components. Runs in each cluster node and makes sure that the cluster configuration file in each cluster node is up to date. In Figure~\ref{fig:ccs} (\small {took from \url{https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/4/html/Cluster_Suite_Overview/images/}}) is represented a CCS overview.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.50]{ccs-overview.png}
  \caption[CCS overview]{CCS overview}
  \label{fig:ccs}
\end{figure}

\begin{itemize}
	\item Cluster Manager (CMAN\footnote{https://www.sourceware.org/cluster/cman/})
\end{itemize}

\noindent A set of kernel patches and a userspace program, formed by a Connection Manager (cnxman) and a Service Manager (sm). The first one handles membership, messaging, quorum, event notification and transitions, and the second one is responsible for instances of external systems. It combines some functionalities provided by CRMsh, PCS and CCS.


%=====================================
% IMPLEMENTATION
%
\chapter{Implementation}
\label{chap:implementation}

This section is intended to provide technical documentation in the process of implementing high availability in a F$\ell$OSS Zimbra Collaboration System (ZCS). The scope of this implementation is framed by the following software components and versions:

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\item Red Hat Enterprise Linux Server release 6.5 (Santiago)
	\item GNU/Linux 2.6.32-431.el6.x86\_64
	\item zcs 8.0.7\_GA\_6021.RHEL6\_64 FOSS edition
	\item drbd 8.4.3-33
	\item corosync 1.4.5-2.2
	\item pacemaker 1.1.10-14
	\item pcs 0.9.90-2
	\item crmsh 1.2.5-0
	\item ccs 0.16.2-69
	\item cman 3.0.12.1-59
\end{itemize}

\noindent The defined cluster consists of two nodes which will be referenced as Astapor and Braavos in the domain got.com (as in the novel Game of Thrones). These nodes are virtual machines hosted on two Proxmox Virtual Environment servers based on KVM virtualization, which are installed on separate physical machines in the same LAN to avoid single point of failure. The proposed scheme is conceptually similar to the observed in Figure~\ref{fig:ha-cluster}.

\FloatBarrier
\begin{figure}
  \centering
  \includegraphics[scale=0.50]{two_nodes_ha_cluster.png}
  \caption[Two nodes HA cluster]{Two nodes HA cluster}
  \label{fig:ha-cluster}
\end{figure}


\section{Operating system considerations}
\label{sec:considerations}

The configuration must be similar in both nodes. In Table~\ref{table:os} is shown the configuration selected for the current implementation. It should be take into consideration that it is not necessary to format partitions for devices vdb1 or vdc1 during OS install.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r l l l | }
  
    \hline    
    \multicolumn{4}{|c|}{\textbf{RHEL 6.5 x86\_64}}\\
    \hline
    Disk Partitions: & /              & 10 Gb  & \\
                     & /boot          & 100 Mb & \\
                     & /opt/zimbra    & 8 Gb   & (/dev/vdb1)\\
                     & drbd meta-data & 150 Mb & (/dev/vdc1)\\
    CPU:             & 1              &        & \\
    RAM:			 & 2 Gb 		  &        & \\
    \hline
  \end{tabular}
\caption{Operating system configuration}
\label{table:os}
\end{table}


\subsection{FQDN hostnames and IP addresses}
\label{sec:fqdn}

Table~\ref{table:fqdn} shows the current configuration for the virtual IP address shared by the two nodes, and for the primary IP address on each node.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r c l | }
    \hline    
    Split DNS IP: & 172.17.18.190 & zcs-ha.got.com \\
	Astapor:      & 172.17.18.191 & astapor.got.com \\
	Braavos:      & 172.17.18.192 & braavos.got.com \\
    \hline
  \end{tabular}
\caption{FQDN hostnames and IP addresses}
\label{table:fqdn}
\end{table}

\noindent On both nodes, /etc/hosts file should contain at least the entries described in Table~\ref{table:hosts}:

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l l l | }
    \hline    
    127.0.0.1     & localhost.localdomain & localhost \\
    127.0.0.1     & zcs-ha.got.com        & zcs-ha\\
	172.17.18.190 & astapor.got.com       & astapor\\
	172.17.18.191 & braavos.got.com       & braavos\\
    \hline
  \end{tabular}
\caption{/etc/hosts file}
\label{table:hosts}
\end{table}

\noindent A useful command to handle hostname changes in RHEL:\\
\indent service hostname restart\bigskip


\subsection{Network}
\label{sec:network}

\begin{itemize}
	\item Internet Protocol version 4 (IPv4)
\end{itemize}

\noindent Set the proper network parameters in /etc/sysconfig/network-scripts/ifcfg-eth0 file on each server, as described in Table~\ref{table:ifcfg}.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l | l | }
    \hline
    \multicolumn{1}{|c|}{Astapor} & Braavos \\
    \hline
    DEVICE=eth0 & DEVICE=eth0 \\
    HWADDR=26:34:99:65:d7:77 & HWADDR=26:34:99:65:d7:78\\
    TYPE=Ethernet & TYPE=Ethernet\\
    ONBOOT=yes & ONBOOT=yes\\
    NM\_CONTROLLED=no & NM\_CONTROLLED=no\\
    BOOTPROTO=none & BOOTPROTO=none\\
    IPADDR=172.17.18.191 & IPADDR=172.17.18.192\\
    NETMASK=255.255.255.0 & NETMASK=255.255.255.0\\
    GATEWAY=172.17.18.1	& GATEWAY=172.17.18.1\\
	DNS1=127.0.0.1 & DNS1=127.0.0.1\\
	IPV6INIT=no & IPV6INIT=no\\
	USERCTL=no & USERCTL=no\\
    \hline
  \end{tabular}
\caption{/etc/sysconfig/network-scripts/ifcfg-eth0 file}
\label{table:ifcfg}
\end{table}


\noindent Set the correct Netmask and Gateway, so servers are able to reach internet addresses, also disable the firewall or allow the http and ftp outgoing rules on it. The primary DNS server will be configured later to be the localhost, with forwarding to external DNS servers.\bigskip

\noindent Some useful commands to manipulate and consult the network service on RHEL:\\
\indent service network restart\\
\indent /etc/init.d/network restart\\
\indent ifconfig eth0 down; ifconfig eth0 up\\
\indent ifdown eth0; ifup eth0\\
\indent ifconfig\\
\indent ip addr show

\begin{itemize}
	\item NTP
\end{itemize}

\noindent Required RPM packages to sincronize cluster nodes through network time protocol: ntp, ntpdate.\bigskip

\noindent Set the proper NTP parameters in /etc/ntp.conf file on each server, so both nodes share the same date and time, as shown in Table~\ref{table:ntp}.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l l | }
    \hline
    driftfile & DEVICE=eth0/var/lib/ntp/drift \\
	restrict & default kod nomodify notrap nopeer noquery \\
    restrict & 127.0.0.1 \\
    server & 172.17.18.1 \\
    includefile & /etc/ntp/crypto/pw \\
    keys & /etc/ntp/keys \\
    \hline
  \end{tabular}
\caption{/etc/ntp.conf file}
\label{table:ntp}
\end{table}


\noindent Some useful commands to manipulate and consult NTP service on RHEL are:\\
\indent service ntpd restart \\
\indent ntpstat \\
\indent ntpq -pn \\
\indent date

\begin{itemize}
	\item BIND
\end{itemize}

\noindent Required RPM packages for domain name resolution: bind, bind-utils.\bigskip

\noindent A primary DNS server configured on each server is crucial, or alternatively a remote centralized DNS server on the LAN with the whole configuration. Here is considered the first option. Table~\ref{table:named} shows the content of /etc/named.conf file.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l | }
    \hline
	  zone ``got.com." IN \{ \\
       type master; \\
       file ``got.com.db"; \\
       \}; \\
    \hline
  \end{tabular}
\caption{/etc/named.conf file}
\label{table:named}
\end{table}


\noindent Astapor node holds /var/named/got.com.db file, with the content described in Table~\ref{table:bindastapor}. Dot characters at the end of hostnames are not a typo, they should be included so that the configuration is correct, and must be absent in the case of IP addresses.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l l l l l | }
    \hline
	                  & IN & 1H & NS    & zcs-ha.got.com.\\
                      & IN & 1H & MX 5  & zcs-ha.got.com.\\
      zcs-ha          & IN & 1H & A     & 172.17.18.190\\
      astapor         & IN & 1H & A     & 172.17.18.191\\
      astapor.got.com & IN &    & CNAME & zcs-ha.got.com.\\
    \hline
  \end{tabular}
\caption{/var/named/got.com.db file}
\label{table:bindastapor}
\end{table}

\noindent A similar got.com.db file must be set on braavos node replacing the corresponding hostname and IP address. Leaving zcs-ha entries without changes in both nodes.\bigskip

\noindent Some useful commands to handle and request BIND service on RHEL are:\\
\indent named-checkconf -z \\
\indent service named restart \\
\indent service named status \\
\indent dig -t ANY got.com \\
\indent nslookup astapor.got.com

\subsection{ZCS dependencies}
\label{sec:zcsdeps}

As requirement for ZCS, the following RPM packages must be installed in the OS:

\begin{itemize}
	\item nc
	\item sudo
	\item libidn 
	\item gmp 
	\item libaio 
\end{itemize}

\noindent Some other suggested RPM packages are:

\begin{itemize}
	\item perl-5.10.1
	\item sysstat 
	\item sqlite 
\end{itemize}

\noindent The postfix daemon must be turned off and excluded from boot start-up:\\
\indent service postfix stop\\
\indent chkconfig postfix off


\section{DRBD}
\label{sec:drbd}

\noindent The Distributed Replicated Block Device (DRBD) provides a mirrored storage required for the HA environment.


\subsection{Initial configuration}
\label{sec:initialconf}

The following actions must be performed in parallel on both nodes, except in those cases where otherwise specified.

\begin{itemize}
	\item Ensure to adapt hostname to `astapor' on the primary node and `braavos' on the secondary node.
	
	\item Install RPM packages:\\
			drbd-kmdl-2.6.32-431.el6-8.4.3-33.el6.x86\_64\\
			drbd-8.4.3-33.el6.x86\_64
	
	\item Leave /etc/drbd.conf and /etc/drbd.d/global\_common.conf files by default.
	
	\item Add /etc/drbd.d/optzimbra.res file with the content described in Table~\ref{table:resource}:

%%%%%%%%%%%%%%%%%%%% MODIFY TABLE %%%%%%%%%%%%%%%%%%%%%%
\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | l l l l l | }
    \hline
      zcs-ha          & IN & 1H & A     & 172.17.18.190\\
      astapor         & IN & 1H & A     & 172.17.18.191\\
    \hline
  \end{tabular}
\caption{/etc/drbd.d/optzimbra.res file}
\label{table:resource}
\end{table}
	
	\item Remove from /etc/fstab file any reference to /dev/vdb1 or /dev/vdc1 devices, as drbd is going to handle its mounting.
	
	\item Initialize data and metadata disks:\\
		\indent dd if=/dev/zero of=/dev/vdb1 bs=1K count=100\\
		\indent dd if=/dev/zero of=/dev/vdc1 bs=1K count=100
	
	\item Start DRBD module:\\
		modprobe drbd
	
	\item Create resource:\\
		drbdadm create-md optzimbra
	
	\item Execute first DRBD synchronisation on astapor:\\
		drbdadm up optzimbra\\
		drbdadm primary --force optzimbra\\
		drbdadm --discard-my-data connect optzimbra

	\item It is possible to check synchronisation status with:\\
		watch cat /proc/drbd

	\item Final output will show:\\
		ds:UpToDate/UpToDate

	\item Verify current roles:\\
		drbdadm role optzimbra\\
		It will show `Primary/Secondary' on astapor \\
		and `Secondary/Primary' on braavos node.

	\item Now make the filesystem on astapor:\\
		mkfs.ext4 /dev/drbd0

	\item Then demote node to secondary, by executing only on astapor:\\
		drbdadm secondary optzimbra

	\item Promote node to primary, by executing only on braavos:\\
		drbdadm primary optzimbra

	\item Make the filesystem on braavos:\\
		mkfs.ext4 /dev/drbd0

\end{itemize}

\noindent Now it is necessary to revert the roles back, making braavos the secondary node and astapor the primary one.


\subsection{DRBD Split Brain Recovery}
\label{sec:splitbrain}

\noindent Assuming that the primary node is still consistent, and the secondary node has an inconsistent state, it would be necessary to recover data loss. The following actions will allow to recover the data corrupted in secondary node.

\begin{itemize}
	\item In both nodes:\\
		drbdadm disconnect optzimbra
	
	\item In the secondary node:\\
		drbdadm secondary optzimbra\\
		drbdadm connect -\ -discard-my-data optzimbra
	
	\item In the primary node:\\
		drbdadm connect optzimbra
	
	\item Finally it is possible to check the sync status, running the command ``cat /proc/drbd", which is going to show a message similar to this:\\
		cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r- \ - \ - \ -

\end{itemize}

\section{ZCS}
\label{sec:zcs}

Here will be fully installed ZCS on astapor but just a dummy installation on braavos, since DRBD will replicate the data to the other node. Download and place ZCS installation file in astapor and braavos filesystems. It can be found at \url{http://www.zimbra.com/downloads/os-downloads.html}. In order to complete a full install on a single server, the following resource will be useful:\\
\url{http://files.zimbra.com/website/docs/8.5/Zimbra_OS_Quick_Start_8.5.0.pdf}


\subsection{ZCS full install on primary node}
\label{sec:fullinstall}

The following actions must be performed sequentially on \textit{\textbf{astapor}}.

\begin{itemize}
	\item Create directory for ZCS:\\
		mkdir   /opt/zimbra
	
	\item Mount DRBD device on ZCS mount point:\\
		mount   /dev/drbd0   /opt/zimbra
	
	\item Check mounted device:\\
		df\  \textbar \ grep zimbra\\
		mount\  \textbar \ grep zimbra
	
	\item Set manual virtual link configuration temporally:\\
		ifconfig eth0:1 inet 172.17.18.190 netmask 255.255.255.0
	
	\item Set split DNS hostname temporally:\\
		hostname zcs-ha.got.com\\
		It is also recommendable to change /etc/sysconfig/network file.
	
	\item Unpack ZCS installer and proceed with full installation:\\
		./install.sh
	
	\item Leave all packages to install by default, and follow the process.
	
	\item When prompted for domain name change, select ``Yes" and then provide: got.com
	
	\item On ``Main Menu" section, set admin user password by browsing through option 3 and then 4:\\
		``Password for admin@zcs-ha.got.com (min 6 characters)"
	
	\item Apply configuration and advance until ZCS setup process is completed:\\
		``Configuration complete - press return to exit"
	
	\item Check ZCS status:\\
		service zimbra status
	
	\item Stop ZCS:\\	
		service zimbra stop
	
	\item Umount DRBD device:\\
		umount   /opt/zimbra
	
	\item Set original DNS hostname:\\
		hostname astapor.got.com\\
		* Revert change in /etc/sysconfig/network file if needed.
	
	\item Delete temporal virtual link configuration:\\
		ifconfig eth0:1 down
	
	\item Demote astapor to secondary DRBD, and continue with section~\ref{sec:dummyinstall}:\\
		drbdadm secondary optzimbra
		
\end{itemize}


\subsection{ZCS dummy install on secondary node}
\label{sec:dummyinstall}

\noindent The following actions must be performed sequentially on braavos.

\begin{itemize}
	\item Promote braavos to primary DRBD:\\
		drbdadm primary optzimbra
		
	\item Create directory for ZCS:\\
		mkdir   /opt/zimbra
		
	\item Mount DRBD device on ZCS mount point:\\
		mount   /dev/drbd0   /opt/zimbra
	
	\item Check mounted device:\\
		df\  \textbar \ grep zimbra\\
		mount\  \textbar \ grep zimbra
		
	\item Unpack ZCS installer and proceed with a dummy installation:\\
		./install.sh -s
		
	\item Stop ZCS:\\
		service zimbra stop
	
	\item Umount DRBD device:\\
		umount   /opt/zimbra
		
	\item Demote braavos back to secondary DRBD:\\
		drbdadm secondary optzimbra
		
	\item Promote astapor back to primary DRBD, executing from astapor node:\\
		drbdadm primary optzimbra
		
\noindent At this point DRBD has to synchronize data from primary node, so check the status until it is done:\\
	watch cat /proc/drbd
	
\end{itemize}

\section{OCF}
\label{sec:ocf}

\noindent Open Cluster Framework, standard scripts to control services such as ZCS. Following actions must be performed in both nodes.

\begin{itemize}
	\item Add btactic zimbra script to /usr/lib/ocf/resource.d/btactic/zimbra. The source code of this script is included in the Appendix~\ref{app:appendix1}.\\

	\item Also create the following symbolic link:\\
		ln \ -s \ /usr/lib/ocf/resource.d/btactic/zimbra \ /usr/lib/ocf/resource.d/heartbeat/
\end{itemize}

\noindent In section~\ref{sec:pacemaker} this file will be referenced.

\section{Pacemaker}
\label{sec:pacemaker}

\noindent Resource manager, starts and stops services orderly.

\begin{itemize}
	\item Install the required RPM packages:\\
		pacemaker-cluster-libs-1.1.10-14.el6.x86\_64\\
		pacemaker-libs-1.1.10-14.el6.x86\_64\\
		pacemaker-cli-1.1.10-14.el6.x86\_64\\
		pacemaker-1.1.10-14.el6.x86\_64\\
		cman-3.0.12.1-59.el6.x86\_64\\
		crmsh-1.2.5-0.el6.x86\_64\\
		ccs-0.16.2-69.el6.x86\_64\\
		resource-agents-3.9.2-40.el6\_5.7.x86\_64
\end{itemize}

\noindent Usually it is difficult to obtain the required RPM's for RHEL, so an alternative is to add CentOS repository by editing /etc/yum.repo.d/centos.repo file with the content described in Table~\ref{table:centosrepo}.

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r c l | }
    \hline
      \multicolumn{3}{|c|}{[centos-6-base]}\\
      \hline
      name &=& CentOS-\$releasever - Base\\
      mirrorlist &=& http://mirrorlist.centos.org/?release=6.5\&arch=x86\_64\&repo=os\\
      enabled &=& 0\\
      gpgcheck &=& 0\\
      baseurl &=& http://mirror.centos.org/centos/6.5/os/x86\_64/\\
    \hline
  \end{tabular}
\caption{/etc/yum.repo.d/centos.repo file}
\label{table:centosrepo}
\end{table}

\begin{itemize}
	\item Then update and install the packages:\\
		yum \ install \ \textminus \textminus enablerepo=centos-6-base \ pacemaker \ pcs.noarch \ cman \textbackslash \\
		ccs \ resource-agents \ crmsh
\end{itemize}


\noindent There are two ways to interact with Pacemaker configuration. The first one is using the crmsh interpreter, starting the crm shell with ``crm" command, and then providing configuration sentences. For instance:\bigskip

	[root@astapor \textasciitilde]\# crm\\
	\indent crm(live)\# help\\
	\indent crm(live)\# quit\\

\noindent Another way would be through \textit{\textbf{pcs}} and \textit{\textbf{ccs}} instructions directly from a linux tty in a bash session. Following is going to be used this way to configure the cluster, executing the commands only on the primary node.

\begin{itemize}
	\item Create the cluster:\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus createcluster \ zcsCluster
\end{itemize}

\begin{itemize}
	\item Add the nodes:\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addnode \ astapor.got.com\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addnode \ astapor.got.com
\end{itemize}

\begin{itemize}
	\item Set fencing to defer to Pacemaker:\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addfencedev \ pcmk \ agent=fence\_pcmk\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addmethod \ pcmk-redirect \ astapor.got.com\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addmethod \ pcmk-redirect \ braavos.got.com\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addfenceinst \ pcmk \ astapor.got.com \textbackslash \\ pcmk-redirect \ port=astapor.got.com\\
		ccs \ \textminus \textminus file \ /etc/cluster/cluster.conf \ \textminus \textminus addfenceinst \ pcmk \ braavos.got.com \textbackslash \\ pcmk-redirect \ port=braavos.got.com
\end{itemize}


\begin{itemize}
	\item Disable CMAN quorum:\\
		This will let the cluster function if only one node is up, and it is necessary to be performed in both nodes.\\
		echo \ ``CMAN\_QUORUM\_TIMEOUT=0" \ \textgreater \textgreater \ /etc/sysconfig/cman
\end{itemize}

\begin{itemize}
	\item Start Pacemaker Cluster:\\
		pcs \ cluster \ start \ \textminus \textminus all\\
		Also equivalent to execute on each node,\\
		``service pacemaker start" or ``pcs cluster start"
\end{itemize}

\begin{itemize}
	\item Copy cluster file to secondary node:\\
		scp \ -p \ /etc/cluster/cluster.conf \ braavos:/etc/cluster/
\end{itemize}

\begin{itemize}
	\item Check Pacemaker cluster status:\\
		pcs \ status\\
		crm\_mon \ -1
\end{itemize}


\begin{itemize}
	\item Show current cluster config:\\
		pcs \ config\\
		pcs \ property\\
		crm \ configure \ show
\end{itemize}


\begin{itemize}
	\item Check configuration validity:\\
		crm\_verify \ -L \ -V
\end{itemize}


\begin{itemize}
	\item Disable STONITH (a type of fencing):\\
		pcs \ property \ set \ stonith-enabled=false
\end{itemize}


\begin{itemize}
	\item Ignore Quorum Policy:\\
		pcs \ property \ set \ no-quorum-policy=ignore
\end{itemize}


\begin{itemize}
	\item Set reconnect attempt:\\
		pcs \ property \ set \ migration-threshold=1 \ \textminus \textminus force
\end{itemize}


\begin{itemize}
	\item Set stickiness:\\
		pcs \ property \ set \ resource-stickiness=100 \ \textminus \textminus force
\end{itemize}

\bigskip

\noindent Now, it is going to be used the crmsh interpreter, starting it with the following command:\\
	\indent crm \ configure

\bigskip
	
\begin{itemize}
	\item Add floating IP address resource (Virtual IP - VIP):\\
		pcs \ resource \ create \ VIP1 \ IPaddr2 \ ip=172.17.18.190 \ broadcast=172.17.18.255 \ \textbackslash \\
			nic=eth0 \ cidr\_netmask=24 \ iflabel=VIP1 \ op \ monitor \ interval=30s \ timeout=30s
\end{itemize}


\begin{itemize}
	\item Define DRBD cluster resource:\\
		configure \ primitive \ drbd \ ocf:linbit:drbd \ params \ \textbackslash \\
			drbd\_resource=optzimbra \ \textbackslash \\
			op \ monitor \ role=Master \ interval=60s \ \textbackslash \\
			op \ monitor \ role=Slave \ interval=50s \ \textbackslash \\
			op \ start \ role=Master \ interval=60s \ timeout=240s \ \textbackslash \\
			op \ start \ role=Slave \ interval=0s \ timeout=240s \ \textbackslash \\
			op \ stop \ role=Master \ interval=60s \ timeout=100s \ \textbackslash \\
			op \ stop \ role=Slave \ interval=0s \ timeout=100s
\end{itemize}

\begin{itemize}
	\item Define DRBD Zimbra data clone:\\
		configure \ ms \ drbd\_ms drbd \ \textbackslash \\
		meta \ master-max=1 \ master-node-max=1 \ \textbackslash \\
		clone-max=2 \ clone-node-max=1 \ notify=true
\end{itemize}

\begin{itemize}
	\item Define Zimbra service resource:\\
		configure \ primitive \ zcs\_service ocf:btactic:zimbra \ \textbackslash \\
		op \ monitor \ interval=2min \ timeout="40s" \ \textbackslash \\
		op \ start \ interval="0" \ timeout="360s" \ \textbackslash \\
		op \ stop \ interval="0" \ timeout="360s"
\end{itemize}

\begin{itemize}
	\item Define Zimbra cluster filesystem resource:\\
		configure \ primitive \ zcs\_fs \ ocf:heartbeat:Filesystem \ params \ \textbackslash \\ 
		device="/dev/drbd0" \ directory="/opt/zimbra" \ fstype=ext4 \ \textbackslash \\
		op \ start \ interval=0 \ timeout=60s \ \textbackslash \\
		op \ stop \ interval="0" \ timeout="60"
\end{itemize}

\begin{itemize}
	\item Group all resources in the same host:\\
		group zcsgroup zcs\_fs zcs\_service \ \textbackslash \\
		configure colocation VIP1-with-drbd\_ms-Master \ inf: drbd\_ms:Master VIP1 \\
		configure colocation drbd\_ms-Master-with-zcs\_fs \ inf: zcs\_fs drbd\_ms:Master \\
		configure colocation zcs\_fs-with-zcs\_service \ inf: zcs\_service zcs\_fs
\end{itemize}

\begin{itemize}
	\item Order resources:\\
		configure order drbd\_ms-promote-on-VIP1 \ inf: VIP1:start drbd\_ms:promote\\
		configure order zcs\_fs-on-dbrb\_ms-promote \ inf: dbrb\_ms:promote zcs\_fs:start\\
		configure order zcs\_service-on-zcs\_fs \ inf: zcs\_fs:start zcs\_service:start
\end{itemize}

\begin{itemize}
	\item Commit configuration changes and quit:\\
		commit\\
		quit
\end{itemize}

\noindent On both nodes make sure chkconfig is off on every service but DRBD. This means the service will not start up on when the server starts up.\\
	\indent chkconfig corosync off\\
	\indent chkconfig cman off\\
	\indent chkconfig ricci off\\
	\indent chkconfig pacemaker off\\
	\indent chkconfig drbd on

\section{Control and check services}
\label{sec:control}


\begin{itemize}
	\item Check Pacemaker cluster status:\\
		crm\_mon -1\\
		pcs status
\end{itemize}

\begin{itemize}
	\item Check resources status:\\
		crm resource status RESOURCE
\end{itemize}

\begin{itemize}
	\item Check configuration validity:\\
		crm\_verify -L -V
\end{itemize}

\begin{itemize}
	\item Edit values already configured:\\
		crm configure edit\\
	After save changes through the preferred text editor, exit and execute:\\
		cibadmin \textminus \textminus replace
\end{itemize}

\begin{itemize}
	\item Delete existent resource:\\
		pcs resource delete RESOURCE
\end{itemize}

\begin{itemize}
	\item Clean resource history errors (check configuration health):\\
		crm\_resource -P
\end{itemize}

\begin{itemize}
	\item List available classes and resources:\\
		crm ra classes\\
		crm ra list ocf btactic\\
		crm ra list lsb
\end{itemize}

\begin{itemize}
	\item Delete cluster configuration (WARNING):\\
		pcs cluster destroy
\end{itemize}

\section{Testing failover}
\label{sec:failover}


\begin{itemize}
	\item On primary node:\\
		crm node standby\\
	Or stop pacemaker:\\
		service pacemaker stop
\end{itemize}


\begin{itemize}
	\item Now ``crm\_mon" or ``pcs status" will show:\\
		Node astapor.got.com: standby\\
		Online: [ braavos.got.com ]
\end{itemize}


\begin{itemize}
	\item It is going to take a while before secondary node takes control. So it is possible to check logs and ``crm\_mon" status during the process.\\
		crm\_mon\\
		tail -F /var/log/zimbra.log\\
		tail -F /var/log/messages
\end{itemize}


\begin{itemize}
	\item Also it is possible to check with ``crm\_standby" command. A value of true\textbar on indicates that the node is not able to host any resources and a value of false\textbar off indicates it does.\\
		crm\_standby \textminus \textminus get-value
\end{itemize}


\begin{itemize}
	\item At any moment it will be displayed a message like the depicted in Table~\ref{table:failover}.
\end{itemize}

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r l | }
    \hline
      Master/Slave Set: 		& 	drbd\_ms [drbd]\\
      Masters: 					& [ braavos.got.com ]\\
      Slaves: 					& [ astapor.got.com ]\\
      Resource Group: 			& zcsgroup\\
      zcs\_fs (ocf::heartbeat:Filesystem): & Started braavos.got.com\\
      zcs\_service (ocf::btactic:zimbra):  & Started braavos.got.com\\
      VIP1 (ocf::heartbeat:IPaddr2):       & Started braavos.got.com\\
    \hline
  \end{tabular}
\caption{DRBD failover test}
\label{table:failover}
\end{table}

\begin{itemize}
	\item Now the secondary node has control of the cluster resources, while the primary node is in standby or unreachable state. If primary node is back online, secondary node will keep the control of resources, until an explicit node move is done.
\end{itemize}


\begin{itemize}
	\item Set back online the primary node:\\
			crm node online\\
		  Or start over pacemaker service:\\
		  	service pacemaker start
\end{itemize}


\begin{itemize}
	\item To give the control back to primary node, execute on secondary node:\\
			crm node standby\\
		  Then resources will be transferred back to primary node.
\end{itemize}


\begin{itemize}
	\item Finally ``crm\_mon" or ``pcs status" on each node will display a similar to the one showed in Table~\ref{table:sync}
\end{itemize}

\FloatBarrier
\begin{table}[H]
  \centering
  \begin{tabular}{ | r l | }
    \hline
      Online:                   & [ astapor.got.com braavos.got.com ]\\
      Master/Slave Set: 		& 	drbd\_ms [drbd]\\
      Masters: 					& [ astapor.got.com ]\\
      Slaves: 					& [ braavos.got.com ]\\
      Resource Group: 			& zcsgroup\\
      zcs\_fs (ocf::heartbeat:Filesystem): & Started astapor.got.com\\
      zcs\_service (ocf::btactic:zimbra):  & Started astapor.got.com\\
      VIP1 (ocf::heartbeat:IPaddr2):       & Started astapor.got.com\\
    \hline
  \end{tabular}
\caption{DRBD synced status}
\label{table:sync}
\end{table}


%=====================================
% RESULTS AND DISCUSSION
%
\chapter{Results and discussion}
\label{chap:results}

In the initial research over internet, main F$\ell$OSS source code repositories were consulted (listed bellow alphabetically) in order to review the state of the art regarding tools that could fit the requirements. Most of the related projects were currently hosted in Github. It is worth noting important work in this area such as the ``Study of available tools"~\cite{FMC} by the FLOSSMetrics Consortium.

\begin{itemize}
	\item BerliOS (berlios.de)
	\item BountySource (bountysource.com)
	\item FLOSSMetrics (flossmetrics.org)
	\item FLOSSmole (ossmole.sourceforge.net)
	\item GitHub (github.com)
	\item Gitorious (gitorious.org)
	\item GNU Savannah (savannah.gnu.org)
	\item Launchpad (launchpad.net)
	\item SourceForge (sourceforge.net)
\end{itemize}

\noindent Since the desired solution was corporate oriented and focused on a GNU/Linux distribution, official websites and documentation were browsed for Hewlett Packard, Redhat Enterprise Linux and SuSE Linux Enterprise Server.

\begin{itemize}
	\item ``HP Serviceguard sglx for Linux Deployment Guide". Hewlett Packard Co.\\
	      DOI=http://www.hp.com/go/sglx/info
	\item ``Red Hat Enterprise Linux 6 Cluster Administration". Red Hat Inc. et al, 2014.\\
	      DOI=https://access.redhat.com/documentation/en-US/Red\_Hat\_Enterprise\_Linux/6/html-single/ \textbackslash \\
	      Cluster\_Administration
	\item Roth T. and Schraitle T. ``SUSE Linux Enterprise High Availability Extension". Novell Inc, 2014.\\
	      https://www.suse.com/documentation\\
	      https://en.opensuse.org/openSUSE:High\_Availability
\end{itemize}

\noindent Also documentation from each individual F$\ell$OSS product:

\begin{itemize}
	\item DRBD: Haas F., Reisner P., Ellenberg L. et al. ``The DRBD User's Guide". LINBIT Information Technologies GmbH and LINBIT HA Solutions GmbH. 2011.\\
	DOU=https://drbd.linbit.com/users-guide
	\item Linux-HA: Haas, Florian. ``The Linux-HA User's Guide". LINBIT HA-Solutions GmbH, The Linux-HA Project. 2010. \\
	DOI=http://www.linux-ha.org/wiki/MainPage
	\item Pacemaker: ``A scalable High Availability cluster resource manager". ClusterLabs. \\
	DOI=http://clusterlabs.org/wiki/MainPage
	\item OCF: Haas, Florian. ``The OCF Resource Agent Developer's Guide". LINBIT HA-Solutions GmbH, Novell, Inc., SUSE Linux GmbH, hastexo Professional Services GmbH. 2011. \\
        DOI=http://www.linux-ha.org/doc/dev-guides/ra-dev-guide.html
\end{itemize}

\noindent Also specific related work was found, such as:

\begin{itemize}
	\item Gibanel Lopez, Adrian. ``Zimbra 8 High Availability on Ubuntu 12.04". Universitat de Lleida. 2013. DOI=http://repositori.udl.cat/bitstream/handle/10459.1/46685/agibanell.pdf
	\item OCF zimbra script: https://github.com/adrian15/hazimbra-thesis/blob/master/ocf/zimbra
	\item Vidal Lopez M. and Castro Jose L. ``Creacion de un cluster de alta disponibilidad con software libre". Novatica Journal. Nro 210. 2011. \\
	DOI=http://www.ati.es/novatica/2011/209/Nv209-75.pdf
\end{itemize}

\noindent Once enough information on the subject of interest have been collected, is crucial to abstract the essence, for which a model as LUM~\cite{CandT1} is ideal. This model focuses on user needs and the demanded effort when selecting a solution to a problem from a set of possible solutions, identifying significant results from patterns and focus on the primary research collected.\bigskip

\noindent A particular type of data that has been obtained is the source code repository for some of the main projects involved in the final solution. They refers to collection of source code used to build a particular software system, supporting versioning through revision control systems, on multi-developer projects to handle various code versions and providing aid in resolving conflicts that arise from developers submitting conflicting modifications.

\begin{itemize}
	\item Corosync: https://github.com/corosync/corosync
	\item DRBD: git://git.drbd.org/drbd-8.4.git
	\item Pacemaker: https://github.com/ClusterLabs/pacemaker
\end{itemize}

\noindent These repositories aside from offering the source code of the software, basically provide data about the project's development behavior and details of the community that makes it possible. This allows for instance, to find out how long has been developing the software and to know if the project is still active, among many other interesting analysis. To achieve this is very useful to use tools like Metrics Grimoire~\cite{GSyC}, obtaining data from project's source code repositories and retrieving information about commits, ticket management, communication in mailing lists, etc. The data is organized and stored into SQL databases that can later be mined for specific patterns or summaries of activity. \bigskip

\noindent In particular Repository Handler and CVSanalY are the tools that have been used to extract the required data. A detailed description for setup and use of these tools can be found in the document ``Analysing Libre Software communities"~\cite{IandR} published by GSyC LibreSoft from Universidad Rey Juan Carlos and Bitergia\footnote{\ http://bitergia.com} team. Part of the analysis consisted into make the following SQL requests:

\begin{itemize}
      % [8.A] Is the project is still active?
      \item Number of commits in the last 6 months, shown in Figure~\ref{fig:fig8-1} for Corosync, Figure~\ref{fig:fig8-2} for DRBD and Figure~\ref{fig:fig8-3} for Pacemaker:
      % SELECT COUNT(s.id) FROM scmlog s, people p WHERE s.committer\_id=p.id AND YEAR(s.date)=2014 AND MONTH(s.date) IN (7,8,9,10,11,12);
\end{itemize}

    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.30]{fig8-1.png}
      \caption[Commits in the last 6 months for Corosync]{Corosync - number of commits in the last 6 months}
      \label{fig:fig8-1}
    \end{figure}
	  
    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.30]{fig8-2.png}
      \caption[Commits in the last 6 months for DRBD]{DRBD - number of commits in the last 6 months}
      \label{fig:fig8-2}
    \end{figure}
    
    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.30]{fig8-3.png}
      \caption[Commits in the last 6 months for Pacemaker]{Pacemaker - number of commits in the last 6 months}
      \label{fig:fig8-3}
    \end{figure}


\noindent The values returned on these queries indicate that indeed each of the projects keep some activity to the date that they were executed (December 2014), in particular Pacemaker with 367 commits, followed by Corosync with 82 commits and finally DRBD with 46 commits during the last six months.\bigskip

\noindent A low amount of commits does not necessarily imply a lack of developer's participation, it can also be related to the maturity of the project and its robust structure, although also to a lack of bug posting from the users community. It could be observed in the following queries that these projects particularly have a small but consistent group of committers.


\begin{itemize}
      % [8.B] Evolution in the number of commits
      \item Evolution in the number of commits per year since the begining of the project, shown in Figure~\ref{fig:fig8-4} for Corosync, Figure~\ref{fig:fig8-5} for DRBD and Figure~\ref{fig:fig8-6} for Pacemaker:
      
      For this purpose was employed a GNU GPL python script {INCLUDE INTO APPENDIX} published by Daniel Izquierdo from GSyC/LibreSoft which uses the following query:\\
      SELECT YEAR(date), MONTH(date), DAY(date), COUNT(*) FROM scmlog GROUP BY YEAR(date), MONTH(date), DAY(date);
\end{itemize}

    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.45]{fig8-4.png}
      \caption[Evolution in the number of commits for Corosync]{Corosync - evolution in the number of commits per year}
      \label{fig:fig8-4}
    \end{figure}
	  
    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.45]{fig8-5.png}
      \caption[Evolution in the number of commits for DRBD]{DRBD - evolution in the number of commits per year}
      \label{fig:fig8-5}
    \end{figure}
    
    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.45]{fig8-6.png}
      \caption[Evolution in the number of commits for Pacemaker]{Pacemaker - evolution in the number of commits per year}
      \label{fig:fig8-6}
    \end{figure}

    
\noindent The incremental progress is evident in the number of commits from the start of each project, being Pacemaker the one with a larger number of commits in a relatively similar period of time. DRBD meanwhile began about four years earlier and presents a consistent growth but below the 1000 commits. It was not until the emergence of projects such as Pacemaker and Corosync that its development is driven, due to the benefits that these tools are capable to offer working together to provide a common goal - high availability.


\begin{itemize}
      % [8.C] Evolution in the number of active committers
      \item Evolution in the number of active committers since the begining of the project, shown in Figure~\ref{fig:barchart_c} for Corosync, Figure~\ref{fig:barchart_d} for DRBD and Figure~\ref{fig:barchart_p} for Pacemaker:

      To produce these bar charts was used a simple python script {INCLUDE INTO APPENDIX} with the following queries:\\
      SELECT year(s.date) FROM scmlog s GROUP BY year(s.date);\\
      SELECT COUNT(DISTINCT(p.id)) FROM scmlog s, people p WHERE s.committer\_id=p.id AND YEAR(s.date)=XXXX;
\end{itemize}

    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.45]{barchart_c.png}
      \caption[Active committers for Corosync]{Corosync - evolution in the number of active committers per year}
      \label{fig:barchart_c}
    \end{figure}
	  
    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.45]{barchart_d.png}
      \caption[Active committers for DRBD]{DRBD - evolution in the number of active committers per year}
      \label{fig:barchart_d}
    \end{figure}
    
    \begin{figure}[H]
      \centering
      \includegraphics[scale=0.45]{barchart_p.png}
      \caption[Active committers for Pacemaker]{Pacemaker - evolution in the number of active committers per year}
      \label{fig:barchart_p}
    \end{figure}


\noindent As histograms show, annually the number of active developers related to Corosync (5 on average) and DRBD (4 on average) is low in comparison with Pacemaker (13 on average). It can be said that DRBD and Pacemaker have tended to keep their average number of active committers, while Corosync has decreased it with the passing of the years.\bigskip

\noindent Increases in the number of committers as of 2009 for Corosync, can be related to the fact that the project was formally announced as independent in 2008~\cite{CRS}, the source code of OpenAIS was refactored and its core infrastructure components were adopted into Corosync, possibly turning it more attractive to developers.\bigskip

\noindent Peaks as the one around 2007 for DRBD is related to the discussions to include the project into the Linux kernel mainline~\cite{ELDRBD}, it is likely that this event captured the attention of developers in the community at the moment, and in fact this actually happened. The last value of 2 active developers in 2005 occurs because it still has not been calculated the activity for the entire year.\bigskip

\noindent In 2008 there is a considerable decrease in the number of active committers for Pacemaker, caused by the splitting of the Linux-HA project~\footnote{http://clusterlabs.org/wiki/Pacemaker\#Project\_History}, but then the community have reacted positively and got involved in the project in a notorious way.\bigskip


\noindent Finally it can be said that the analysis suggested by Carlo Daffara~\cite{Daffara1} through his guidelines for the adoption of F$\ell$OSS within SMEs has proved tremendously useful in the process of selecting and evaluate new tools among the vast amount of existent alternatives in the market.\bigskip

\noindent The proposed hypothesis of a search method by collecting and read as much information related to the project is available, results on the one hand, a task very well supported with internet approachability and lots of information available for both official and unofficial. Though on the other hand, this amount of information is not always available in an organized or easy way to digest, leading to an extensive analysis on many occasions that the companies are not willing to invest.\bigskip

\noindent To deal with the latter fact, the Lazy User Model leverages the process by which are elected the technological tools that represent the appropriate solution and fulfill the user requirements. 



%=====================================
% CONCLUSIONS
%
\chapter{Conclusions and future work}
\label{chap:conclusions}

Text..

As future work:
- OpenBRR~\cite{Wasserman}, QSoS, QualOSS, 





%=====================================
% BIBLIOGRAPHY
%
\bibliography{ref}{}
\bibliographystyle{plain}


%=====================================
% APPENDIX
%
\appendix
\chapter{btactic zimbra script}
\label{app:appendix1}
% btactic zimbra script

\#!/bin/sh\\
\#\\
\# Resource script for Zimbra\\
\#\\
\# Description:  Manages Zimbra as an OCF resource in an high-availability setup.\\
\#\\
\# Author:       Adrian Gibanel\\
\# \textless adrian.gibanel@btactic.com\textgreater : Original Author\\
\#\\
\# License: GNU General Public License (GPL)\\
\# Note:  Aimed at an active/passive cluster originally\\
\#        Inspired from postfix OCF script\\
\#        Inspired from Ubuntu LSB script.\\
\#        Not sure it will work for other distros without modifying\\
\#\\
\#   usage: \$0 \{start\textbar stop\textbar reload\textbar status\textbar monitor\textbar validate-all\textbar meta-data\}\\
\#\\
\#       The ``start" arg starts Zimbra\\
\#       The ``stop" arg stops it.\\
\#\\
\# OCF parameters:\\
\#  OCF\_RESKEY\_binary\\
\#  OCF\_RESKEY\_config\_dir\\
\#  OCF\_RESKEY\_parameters\\
\#\\
\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\\

\noindent \# Initialization:\\

\noindent : \$\{OCF\_FUNCTIONS\_DIR=\$\{OCF\_ROOT\}/lib/heartbeat\}\\
. \$\{OCF\_FUNCTIONS\_DIR\}/ocf-shellfuncs\\
: \$\{OCF\_RESKEY\_binary=``zmcontrol"\}\\
: \$\{OCF\_RESKEY\_zimbra\_dir=``/opt/zimbra"\}\\
: \$\{OCF\_RESKEY\_zimbra\_user=``zimbra"\}\\
: \$\{OCF\_RESKEY\_zimbra\_group=``zimbra"\}\\
USAGE=``Usage: \$0 \{start\textbar stop\textbar reload\textbar status\textbar monitor\textbar validate-all\textbar meta-data\}";\\

\noindent \#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\\

\noindent usage() \{\\
    \indent echo \$USAGE \textgreater\&2\\
\}\\

\noindent meta\_data() \{\\

        cat \textless \textless END\\
        
\textless ?xml version=``1.0"?\textgreater\\
\textless !DOCTYPE resource-agent SYSTEM ``ra-api-1.dtd"\textgreater\\
\textless resource-agent name=``zimbra"\textgreater\\
\textless version\textgreater0.1\textless /version\textgreater\\
\textless longdesc lang=``en"\textgreater\\
This script manages Zimbra as an OCF resource in a high-availability setup.\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater\\
Manages a highly available Zimbra mail server instance\\
\textless /shortdesc\textgreater\\

\noindent \textless parameters\textgreater\\

\noindent \textless parameter name=``binary" unique=``0" required=``0"\textgreater\\
\textless longdesc lang=``en"\textgreater\\
Short name to the Zimbra control script.\\
For example, ``zmcontrol".\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater\\
Short name to the Zimbra control script\textless /shortdesc\textgreater\\
\textless content type=``string" default=``zmcontrol" /\textgreater\\
\textless /parameter\textgreater\\

\noindent \textless parameter name=``zimbra\_dir" unique=``1" required=``0"\textgreater\\
\textless longdesc lang=``en"\textgreater\\
Full path to Zimbra directory.\\
For example, ``/opt/zimbra".\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater\\
Full path to Zimbra directory\textless /shortdesc\textgreater\\
\textless content type=``string" default=``/opt/zimbra" /\textgreater\\
\textless /parameter\textgreater\\

\noindent \textless parameter name=``zimbra\_user" unique=``1" required=``0"\textgreater\\
\textless longdesc lang=``en"\textgreater\\
Zimbra username.\\
For example, ``zimbra".\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater Zimbra username\textless /shortdesc\textgreater\\
\textless content type=``string" default=``zimbra" /\textgreater\\
\textless /parameter\textgreater\\

\noindent \textless parameter name=``zimbra\_group" unique=``1" required=``0"\textgreater\\
\textless longdesc lang=``en"\textgreater\\
Zimbra group.\\
For example, ``zimbra".\\
\textless /longdesc\textgreater\\
\textless shortdesc lang=``en"\textgreater Zimbra group\textless /shortdesc\textgreater\\
\textless content type=``string" default=``zimbra" /\textgreater\\
\textless /parameter\textgreater\\

\noindent \textless /parameters\textgreater\\

\noindent \textless actions\textgreater\\
\textless action name=``start"   timeout=``360s" /\textgreater\\
\textless action name=``stop"    timeout=``360s" /\textgreater\\
\textless action name=``restart"  timeout=``360s" /\textgreater\\
\textless action name=``monitor" depth=``0"  timeout=``40s" interval=``60s" /\textgreater\\
\textless action name=``validate-all"  timeout=``360s" /\textgreater\\
\textless action name=``meta-data"  timeout=``5s" /\textgreater\\
\textless /actions\textgreater\\
\textless /resource-agent\textgreater\\

\noindent END\\
\noindent \}\\

\noindent command()\\
\{\\
  \indent if [ -f \$\{zimbra\_dir\}/redolog/redo.log ]; then\\
  \indent \indent chown -f \$\{zimbra\_user\}:\$\{zimbra\_group\} \$\{zimbra\_dir\}/redolog/redo.log\\
  \indent fi\\
  \indent su - \$\{zimbra\_user\} -c ``\$\{binary\} \$1 \textless /dev/null"\\
\}\\

\noindent running() \{\\
\indent \# run Zimbra status\\
\indent  command status\\
\}\\


\noindent zimbra\_status()\\
\{\\
    \indent running\\
\}\\

\noindent zimbra\_start()\\
\{\\
    \indent \# if Zimbra is running return success\\
    \indent if zimbra\_status; then\\
    \indent \indent ocf\_log info ``Zimbra is already running."\\
    \indent \indent return \$OCF\_SUCCESS\\
    \indent fi\\

    \indent \# start Zimbra\\
    \indent command start\\
    \indent ret=\$?\\
    \indent if [ -d /var/lock/subsys -a \$ret -eq 0 ]; then\\
    \indent \indent touch /var/lock/subsys/zimbra\\
    \indent fi\\

    \indent if [ \$ret -ne 0 ]; then\\
        \indent \indent ocf\_log err ``Zimbra returned an error." \$ret\\
        \indent \indent return \$OCF\_ERR\_GENERIC\\
    \indent fi\\

    \indent \# grant some time for startup/forking the sub processes\\
    \indent sleep 2\\

    \indent \# initial monitoring action\\
    \indent running\\
    \indent ret=\$?\\
    \indent if [ \$ret -ne \$OCF\_SUCCESS ]; then\\
     \indent \indent ocf\_log err ``Zimbra failed initial monitor action." \$ret\\
     \indent \indent return \$OCF\_ERR\_GENERIC\\
    \indent fi\\

    \indent ocf\_log info ``Zimbra started."\\
    \indent return \$OCF\_SUCCESS\\
\}\\


\noindent zimbra\_stop()\\
\{\\
    \indent \# if Zimbra is not running return success\\
    \indent if ! zimbra\_status; then\\
        \indent \indent ocf\_log info ``Zimbra already stopped."\\
        \indent \indent return \$OCF\_SUCCESS\\
    \indent fi\\

    \indent \# stop Zimbra\\
    \indent command stop \\
    \indent ret=\$?\\

    \indent if [ -d /var/lock/subsys -a \$ret -eq 0 ]; then\\
      \indent \indent rm -f /var/lock/subsys/zimbra\\
    \indent fi\\

    \indent if [ \$ret -ne 0 ]; then\\
        \indent \indent ocf\_log err ``Zimbra returned an error while stopping." \$ret\\
        \indent \indent return \$OCF\_ERR\_GENERIC\\
    \indent fi\\

    \indent \# grant some time for shutdown and recheck 5 times\\
    \indent for i in 1 2 3 4 5; do\\
        \indent \indent if zimbra\_status; then\\
            \indent \indent \indent sleep 1\\
        \indent \indent fi\\
    \indent done\\

    \indent \# escalate to abort if we did not stop by now\\
    \indent if zimbra\_status; then\\
        \indent \indent ocf\_log err ``Zimbra failed to stop. Escalating to `abort'."\\

        \indent \indent ORPHANED=\textasciigrave ps -u \$\{zimbra\_user\} -o ``pid="` \&\& kill -9 \$ORPHANED 2\textgreater\&1\\
        \indent \indent ret=\$?\\
        \indent \indent sleep 10\\

        \indent \indent \# zimbra abort did not succeed\\
        \indent \indent if zimbra\_status; then\\
            \indent \indent \indent ocf\_log err ``Zimbra failed to abort."\\
            \indent \indent \indent return \$OCF\_ERR\_GENERIC\\
        \indent \indent fi\\
    \indent fi\\

    \indent ocf\_log info ``Zimbra stopped."\\
    \indent return \$OCF\_SUCCESS\\
\}\\

\noindent zimbra\_restart()\\
\{\\
    \indent if zimbra\_status; then\\
        \indent \indent ocf\_log info ``Reloading Zimbra."\\
        \indent \indent command restart\\
    \indent fi\\
\}\\

\noindent zimbra\_monitor()\\
\{\\
    \indent if zimbra\_status; then\\
        \indent \indent return \$OCF\_SUCCESS\\
    \indent fi\\
    \indent return \$OCF\_NOT\_RUNNING\\
\}\\

\noindent zimbra\_validate\_all()\\
\{\\
    \indent \# check zimbra\_dir parameter\\
    \indent if [ ! -d ``\$zimbra\_dir" ]; then\\
      \indent \indent ocf\_log err ``Zimbra directory `\$config\_dir' does not exist." \$ret\\
      \indent \indent return \$OCF\_ERR\_INSTALLED\\
    \indent fi\\
    \indent \# check that the Zimbra binaries exist and can be executed\\
    \indent if ! have\_binary ``\$\{zimbra\_dir\}/bin/\$\{binary\}" ; then\\
      \indent \indent return \$OCF\_ERR\_INSTALLED\\
    \indent fi\\

    \indent \# check permissions\\
    \indent user=\$\{zimbra\_user\}\\
    \indent zimbra\_writable\_dirs=``\$\{zimbra\_dir\}/conf"\\
    \indent for dir in ``\$zimbra\_writable\_dirs"; do\\
        \indent \indent if ! su -s /bin/sh - \$user -c ``test -w \$dir"; then\\
            \indent \indent \indent ocf\_log err ``Directory `\$dir' is not writable by user `\$user'."\\
            \indent \indent \indent exit \$OCF\_ERR\_PERM;\\
        \indent \indent fi\\
    \indent done\\

    \indent return \$OCF\_SUCCESS\\
\}\\

\noindent \#\\
\# Main\\
\#\\

\noindent if [ \$\# -ne 1 ]; then\\
    \indent usage\\
    \indent exit \$OCF\_ERR\_ARGS\\
fi\\

\noindent binary=\$OCF\_RESKEY\_binary\\
zimbra\_dir=\$OCF\_RESKEY\_zimbra\_dir\\
zimbra\_user=\$OCF\_RESKEY\_zimbra\_user\\
zimbra\_group=\$OCF\_RESKEY\_zimbra\_group\\
parameters=\$OCF\_RESKEY\_parameters\\

\noindent \# build Zimbra options string *outside* to access from each method\\
OPTIONS=`'\\
OPTION\_CONFIG\_DIR=`'\\

\noindent \# check if the Zimbra config\_dir exist\\
if [ ``x\$config\_dir" != ``x" ]; then\\
    \indent \# check for postconf binary\\
    \indent \#check\_binary ``\$\{zimbra\_dir\}/bin/\$\{binary\}"\\

    \# remove all trailing slashes\\
    \indent zimbra\_dir=\textasciigrave echo \$zimbra\_dir \textbar \ sed `s/\/*\$//'\textasciigrave \\
fi\\

\noindent case \$1 in\\
    \indent meta-data)  meta\_data\\
                \indent \indent \indent \indent exit \$OCF\_SUCCESS\\
                \indent \indent \indent \indent ;;\\

    \indent usage\textbar help) usage\\
                \indent \indent \indent \indent exit \$OCF\_SUCCESS\\
                \indent \indent \indent \indent ;;\\
\noindent esac\\

\noindent zimbra\_validate\_all\\
ret=\$?\\

\noindent LSB\_STATUS\_STOPPED=3\\
if [ \$ret -ne \$OCF\_SUCCESS ]; then\\
    \indent case \$1 in\\
    \indent \indent stop)       exit \$OCF\_SUCCESS ;;\\
    \indent \indent monitor)    exit \$OCF\_NOT\_RUNNING;;\\
    \indent \indent status)     exit \$LSB\_STATUS\_STOPPED;;\\
    \indent \indent *)          exit \$ret;;\\
    \indent esac\\
fi\\

\noindent case \$1 in\\
    \indent monitor)    zimbra\_monitor\\
                \indent \indent exit \$?\\
                \indent \indent ;;\\
    \indent start)      zimbra\_start\\
                \indent \indent exit \$?\\
                \indent \indent ;;\\

    \indent stop)       zimbra\_stop\\
                \indent \indent exit \$?\\
                \indent \indent ;;\\

    \indent restart)     zimbra\_restart\\
                \indent \indent exit \$?\\
                \indent \indent ;;\\

    \indent status)     if zimbra\_status; then\\
                    \indent \indent \indent \indent ocf\_log info ``Zimbra is running."\\
                    \indent \indent \indent \indent exit \$OCF\_SUCCESS\\
                \indent \indent \indent else\\
                    \indent \indent \indent \indent ocf\_log info ``Zimbra is stopped."\\
                    \indent \indent \indent \indent exit \$OCF\_NOT\_RUNNING\\
                \indent \indent \indent fi\\
                \indent \indent \indent ;;\\

    \indent validate-all)   exit \$OCF\_SUCCESS;;\\

    \indent *)          usage\\
                \indent \indent exit \$OCF\_ERR\_UNIMPLEMENTED;;\\
esac

\end{document}